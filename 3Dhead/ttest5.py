# 加载obj模型
# !/usr/bin/env python
import cv2
import vtk
import vtkmodules
from vtkmodules.vtkIOGeometry import vtkOBJReader
from vtkmodules.vtkFiltersCore import vtkDecimatePro
from vtkmodules.vtkCommonDataModel import vtkPolyData
from vtkmodules.vtkCommonExecutionModel import vtkPolyDataAlgorithm

import mediapipe as mp
from vtkmodules.qt.QVTKRenderWindowInteractor import QVTKRenderWindowInteractor
import sys
# IMPORTING ALL THE NECESSERY PYSIDE2 MODULES FOR OUR APPLICATION.
from PyQt5 import QtCore, QtGui, QtWidgets
from PyQt5.QtCore import *
from PyQt5.QtGui import *
from PyQt5.QtWidgets import *

from ui_tmsvison import Ui_MainWindow  # MAINWINDOW CODE GENERATED BY THE QT DESIGNER AND pyside2-uic.
# from ttestkinect import Camera
# from ttestur3 import UR3
from ttestorbbec import Camera
from Filters import MeanFilter, WeightedFilter  # 滤波器
import math
from scipy.spatial.transform import Rotation as SSTR
# import matplotlib
from TCPServer import TCPServer
import numpy as np
import threading
import time

USEROBOT = True
USEHAND = False
USEFACE = True

rxmin, rxmax = 0.30, 0.850
rymin, rymax = -0.25, 0.30
rzmin, rzmax = -0.20, 0.60

# 水平和垂直翻转后的roi图像区域
cxmin, cxmax = 110, 475
cymin, cymax = 150, 475

# 面向患者视角的左右眼， 图像的左右
flame_left_eyes = [i - 1 for i in [23]]
flame_right_eyes = [i - 1 for i in [26]]
flame_noses = [i - 1 for i in [14]]
flame_mouths = [i - 1 for i in [35]]
#  FLAME 的id 转化到 mediapipe 的id
flame2mps = [70, 63, 105, 66, 107, 336, 296, 334, 293, 300, 168, 6, 195, 4, 98, 97, 2, 326, 327, 33, 159, 157, 133, 153,
             144, 362, 384, 386, 263, 373, 380, 61, 39, 37, 0, 267, 269, 291, 321, 314, 17, 84, 181, 78, 82, 13, 312,
             308, 317, 14, 87]
#                     '1   2   3    4    5    6    7    8    9   10   11   12  13  14  15  16 17  18   19   20  21   22   23   24   25   26   27   28   29   30   31  32  33  34  35  36  37   38   39   40   41  42  43  44  45  46  47   48   49   50  51'

important_ids = [5, 6, 7, 8, 9, 10, 11, 12, 13, 22, 25, 26, 27, 28, 29, 30, 33, 35, 39, 41]


# 信号发送
class MySignal(QObject):
    str_msg = pyqtSignal(QLabel, str)
    ndarray_msg = pyqtSignal(np.ndarray)
    progressbar_int_msg = pyqtSignal(QProgressBar, int)
    int_msg = pyqtSignal(int)
    imgviewer_msg = pyqtSignal(QLabel, QImage)
    model_msg = pyqtSignal(QWidget, int)


class Model(threading.Thread):
    def __init__(self):
        threading.Thread.__init__(self)
        self.ca = Camera("D:/windows/SDK/x64/Redist")
        self.ca.start()
        # USE UR3Robot
        if USEROBOT:
            # self.robot = UR3("192.168.1.3", track_threshold=0.05, payload=0.0)
            # self.robot.start()
            # time.sleep(0.1)
            # print("启动机器人")
            self.TCPServer = TCPServer()
            self.TCPServer.start()
            print("启动TCPServer")

        self.robot_can_move = False
        # self.robot_init_pos = np.array([283.0, 0.0, 210.0, 0.0, 180.0, 0.0])
        self.filter_headx = MeanFilter(20)
        self.filter_nose = MeanFilter(20)
        self.filter_test = MeanFilter(10)
        # self.filter.add(self.robot_init_pos)
        self.Flag = True  # 开启标记位

        # mp config
        self.mp_drawing = mp.solutions.drawing_utils  # mp 绘画对象
        self.mp_face_mesh = mp.solutions.face_mesh  # mp 脸部点集
        self.drawing_spec = self.mp_drawing.DrawingSpec(thickness=1, circle_radius=1)  # config mp 绘画设置

        if USEHAND:
            self.mpHands = mp.solutions.hands
            self.hands = self.mpHands.Hands()  # 设置参数，详见 hands.py 中的 __init__
            self.mpDraw = mp.solutions.drawing_utils  # 将检测出的手上的标记点连接起来

    def robot_into_follow(self):
        if USEROBOT:
            self.robot_can_move = True
            self.TCPServer.canrun = True

    def robot_exit_follow(self):
        if USEROBOT:
            self.robot_can_move = False
            self.TCPServer.canrun = False

    def stop(self):
        self.Flag = False
        self.ca.stop_orbbec()
        if USEROBOT:
            self.TCPServer.stop()

    # 脸部识别主程序
    def face_mark(self, image):
        scale = 5  # 放大roi区域识别人脸
        image_roi = image[cymin:cymax, cxmin:cxmax, :]
        h, w, c = image_roi.shape
        image_roi = cv2.resize(image_roi, (w * scale, h * scale))
        with self.mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5) as face_mesh:
            # image.flags.writeable = False
            results = face_mesh.process(image_roi)  # 对ROI区域进行人脸特征识别
            # image.flags.writeable = True
            if not results.multi_face_landmarks:
                print("未识别人脸")
                return None, None, image
            for face_landmarks in results.multi_face_landmarks:
                h, w, c = image_roi.shape
                h /= scale
                w /= scale
                points = [np.array([0, 0, 0]) for i in range(len(flame2mps))]  # flame的51个点在相机下的坐标
                err = False
                for i in range(len(flame2mps)):
                    lm = face_landmarks.landmark[flame2mps[i]]
                    cx, cy = int(lm.x * w) + cxmin, int(lm.y * h) + cymin
                    if 15 <= i + 1 <= 19:
                        cy -= 2  # 鼻子周围的点下移
                    cv2.circle(image_roi, [int(lm.x * w * scale), int(lm.y * h * scale)], 1 * scale, (255, 0, 0),
                               2 * scale)
                    p = self.ca.color2point([cx, cy], vflip=False, hflip=False)  # vflip，hflip 水平和垂直翻转标志
                    points[i] = p
                    if i in important_ids:
                        if np.all(p == 0) or np.any(p > 1400):
                            err = True
                image_roi = cv2.resize(image_roi, (int(w), int(h)))
                self.mp_drawing.draw_landmarks(
                    image=image_roi,
                    landmark_list=face_landmarks,
                    connections=self.mp_face_mesh.FACE_CONNECTIONS,
                    landmark_drawing_spec=self.drawing_spec,
                    connection_drawing_spec=self.drawing_spec)
                image[cymin:cymax, cxmin:cxmax, :] = image_roi
                print("draw")
                if err:
                    print("关键点数据异常")
                    return None, None, image

                left_eye_pos = np.array([0.0, 0.0, 0.0])
                right_eye_pos = np.array([0.0, 0.0, 0.0])
                nose_pos = np.array([0.0, 0.0, 0.0])
                mouth_pos = np.array([0.0, 0.0, 0.0])

                for id in flame_left_eyes:  # 眼部定位为标记区域的均值中心
                    left_eye_pos += points[id]
                left_eye_pos /= len(flame_left_eyes)

                for id in flame_right_eyes:
                    right_eye_pos += points[id]
                right_eye_pos /= len(flame_right_eyes)

                for id in flame_noses:
                    nose_pos += points[id]
                nose_pos /= len(flame_noses)
                self.filter_nose.add(nose_pos)
                nose_pos = self.filter_nose.get()

                for id in flame_mouths:
                    mouth_pos += points[id]
                mouth_pos /= len(flame_mouths)

                # 求旋转平移矩阵 X：左眼到右眼，Y：嘴角到眼角
                # print(right_eye_pos, left_eye_pos)
                headX = left_eye_pos - right_eye_pos
                self.filter_headx.add(headX)
                headX = self.filter_headx.get()

                headX = headX / np.linalg.norm(headX)  # 归一化

                vet36_26 = points[26 - 1] - points[36 - 1]  # Y　鼻子到眼睛
                vet34_23 = points[23 - 1] - points[34 - 1]
                headY = vet36_26 + vet34_23
                headY = headY - np.dot(headY, headX) * headX
                headY /= np.linalg.norm(headY)

                headZ = np.cross(headX, headY)
                headZ = headZ / np.linalg.norm(headZ)

                # 人头坐标系原点沿X，Y，Z平移
                T_head2ca = nose_pos + 0 * headX + 20 * headY - 90 * headZ
                # print("x y z:", headX, headY, headZ)
                R_head2ca = np.vstack([headX, headY, headZ]).T
                R_ca2head = np.linalg.inv(R_head2ca)
                T_ca2head = -np.dot(R_ca2head, T_head2ca)
                # print("ch: ", T_ca2head,"hc ", T_head2ca)
                id = 11  # 测试id
                p = points[id - 1]
                # print(p)
                # print("左右眼鼻口：", left_eye_pos, right_eye_pos, nose_pos, mouth_pos)
                p = np.dot(R_ca2head, p) + T_ca2head
                # print(p)

                if abs(p[0]) > 20:
                    print("人脸识别误差大")
                    return None, None, image

                return R_head2ca, T_head2ca, image

            return None, None, image

    def get_handpos(self, img):
        """
        输入：rgb_img
        输出：手的位置--相机坐标系，法向量--相机坐标系，手的图像
        """
        # imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        image_roi = img[cymin:cymax, cxmin:cxmax, :]
        results = self.hands.process(image_roi)  # 对输入图像进行处理，探索图像中是否有手
        h, w, c = image_roi.shape
        if results.multi_hand_landmarks:
            for handLms in results.multi_hand_landmarks:  # 捕捉画面中的每一只手
                # 取0, 5, 13 三个关节计算手坐标系
                id = 0
                lm = handLms.landmark[id]
                # for id, lm in enumerate(handLms.landmark):
                cx, cy = int(lm.x * w) + cxmin, int(lm.y * h) + cymin  # 根据比例还原出每一个标记点的像素坐标
                p0 = self.ca.color2point([cx, cy], vflip=False, hflip=False)  # 根据像素点 找到相机坐标系下的坐标
                if p0 is None or any(np.isnan(p0)) or any(np.isinf(p0)):
                    continue
                # p0[2] += 0.010  # 0 节点太翘

                id = 5
                lm = handLms.landmark[id]
                # for id, lm in enumerate(handLms.landmark):
                cx, cy = int(lm.x * w) + cxmin, int(lm.y * h) + cymin  # 根据比例还原出每一个标记点的像素坐标
                p5 = self.ca.color2point([cx, cy], vflip=False, hflip=False)
                if p5 is None or any(np.isnan(p5)) or any(np.isinf(p5)):
                    continue

                id = 9
                lm = handLms.landmark[id]
                # for id, lm in enumerate(handLms.landmark):
                cx, cy = int(lm.x * w) + cxmin, int(lm.y * h) + cymin  # 根据比例还原出每一个标记点的像素坐标
                p9 = self.ca.color2point([cx, cy], vflip=False, hflip=False)
                if p9 is None or any(np.isnan(p9)) or any(np.isinf(p9)):
                    continue

                id = 12
                lm = handLms.landmark[id]
                # for id, lm in enumerate(handLms.landmark):
                cx, cy = int(lm.x * w) + cxmin, int(lm.y * h) + cymin  # 根据比例还原出每一个标记点的像素坐标
                p12 = self.ca.color2point([cx, cy], vflip=False, hflip=False)
                if p12 is None or any(np.isnan(p12)) or any(np.isinf(p12)):
                    continue

                id = 13
                lm = handLms.landmark[id]
                # for id, lm in enumerate(handLms.landmark):
                cx, cy = int(lm.x * w) + cxmin, int(lm.y * h) + cymin  # 根据比例还原出每一个标记点的像素坐标
                p13 = self.ca.color2point([cx, cy], vflip=False, hflip=False)
                if p13 is None or any(np.isnan(p13)) or any(np.isinf(p13)):
                    continue

                # hand_nomal = p12 - p0

                p913 = (p9 + p13) / 2
                hand_pos = (p0 + p5 + p13) / 3
                hand_nomal = p913 - p5
                if hand_nomal is None or not any(hand_nomal):
                    continue
                hand_nomal /= np.linalg.norm(hand_nomal)
                # print("hp", hand_pos)
                # print("hn", hand_nomal)
                self.mpDraw.draw_landmarks(image_roi, handLms, self.mpHands.HAND_CONNECTIONS)  # 给画面中的每一只手进行标点、连线的操作
                img[cymin:cymax, cxmin:cxmax, :] = image_roi
                # print('hand_nomal\n',hand_nomal)
                return hand_pos, hand_nomal, img
        return None, None, img

    def run(self):
        time.sleep(1)
        # while self.Flag:
        #     if self.flag_fitting_face and self.lmk is not None:
        #
        #         print("done")
        #     time.sleep(0.1)
        # print("model stop")


class Controller(threading.Thread):

    def __init__(self):
        threading.Thread.__init__(self)
        self.model = Model()
        self.view = MainWindow()
        # 绑定按钮事件
        self.view.ui.pb1.clicked.connect(self.start_robot)
        self.view.ui.pb2.clicked.connect(self.start_fit)
        # self.view.ui.pb3.clicked.connect(self.openFile)
        self.view.ui.pb4.clicked.connect(self.stop)
        self.view.ui.pushButton_6.clicked.connect(self.readdata)

        self.head_pos = None
        self.head_normal = None
        # 设置追踪距离
        self.track_distance = 30  # unit: mm

        self.Flag = True
        self.view.show()
        self.R = np.array([[0.14641838, 0.02288049, -0.98895811],
                           [-0.98912527, 0.01742143, -0.14604007],
                           [0.01388759, 0.9995864, 0.02518249]])
        self.T = np.array([[1667.51508155],
                           [199.39488522],
                           [-88.64221832]])

        self.model.start()
        time.sleep(1)

    def start_robot(self):
        if not self.model.robot_can_move:
            if USEROBOT:
                self.model.robot_into_follow()
            self.view.ui.pb1.setText("结束跟踪")
        else:
            if USEROBOT:
                self.model.robot_exit_follow()
            self.view.ui.pb1.setText("启动跟踪")

    def start_fit(self):
        self.model.flag_fitting_face = True

    def stop(self):
        self.Flag = False
        self.view.close()
        self.model.stop()

    def readdata(self):
        posx = self.view.ui.pos_x.text()
        posy = self.view.ui.pos_y.text()
        posz = self.view.ui.pos_z.text()
        norx = self.view.ui.nor_x.text()
        nory = self.view.ui.nor_y.text()
        norz = self.view.ui.nor_z.text()
        dist = self.view.ui.tms2head.text()
        # if "" not in [posx, posy, posz, norx, nory, norz]:
        try:
            self.head_pos = np.array([float(posx), float(posy), float(posz)])
            self.head_normal = np.array([float(norx), float(nory), float(norz)])
            self.track_distance = float(dist)
            if self.track_distance <= 30:  # min_max_track_distance = 30mm 120mm
                self.track_distance = 30
            if self.track_distance > 200:
                self.track_distance = 200
            print("跟踪：", self.head_pos, self.head_normal, self.track_distance)
        except:
            print("输入错误")
            self.head_pos = None
            self.head_normal = None
            self.track_distance = 30

    def run(self):

        time.sleep(1.5)

        while self.Flag:
            time.sleep(0.03)
            if self.view.isclosed:
                self.model.stop()
                break

            color_img = self.model.ca.get_img("c")
            if color_img is None or not np.any(color_img):
                continue

            color_img = cv2.cvtColor(color_img[:, :, :3],
                                     cv2.COLOR_BGR2RGB)  # convert an image from one color space to another
            # color_img = cv2.flip(color_img, -1)
            # flip > 0 水平翻转 ,flip =0 垂直翻转 ,flip< 0水平和垂直翻转
            if USEFACE:
                # R_head2ca, T_head2ca, color_img = self.model.face_mark(color_img)
                R_head2ca, T_head2ca, color_img = self.model.face_mark(color_img)
                # print("0000",R_head2ca)
                if R_head2ca is not None and self.head_pos is not None and self.head_normal is not None:
                    # print("hp, hn ",self.head_pos, self.head_normal)
                    cam_head_pos = np.dot(R_head2ca, self.head_pos) + T_head2ca  # 计算标记点在相机坐标系下的位置
                    cam_head_normal = np.dot(R_head2ca, self.head_normal)
                    cam_head_normal /= np.linalg.norm(cam_head_normal)  # 归一化
                    # print("cp, cn ", cam_head_pos, cam_head_normal)
                    self.view.showCameraPos(cam_head_pos)

                    robot_pos = np.dot(self.R, cam_head_pos) + self.T.reshape(1, 3)
                    # print("robot_pos ", robot_pos)
                    robot_pos = robot_pos[0] / 1000

                    # print("robot_pos", robot_pos)
                    self.view.showRobotPos(robot_pos)

                    robot_normal = np.matmul(self.R, cam_head_normal)

                    if robot_normal[2] > 0:
                        # 朝下
                        robot_normal = -robot_normal
                    print("robot_normal", robot_normal)
                    theta = math.pi * 60 / 180  # 若 normal 倾斜超过 theta, 矫正到 theta 内
                    robot_z_vector = np.array([0, 0, -1])
                    if robot_z_vector.dot(robot_normal) < math.cos(theta):
                        nx, ny = robot_normal[0], robot_normal[1]
                        al = math.sin(theta)
                        l = math.sqrt(nx * nx + ny * ny)
                        x, y = al / l * nx, al / l * ny
                        a = np.array([x, y, -math.cos(theta)])
                        robot_normal = a / np.linalg.norm(a)
                    #   Robot EE 法向量
                    # 末端旋转矩阵
                    TZ = robot_normal
                    # print('TZ[0]: ', TZ[0], 'TZ[1]: ', TZ[1])
                    if abs(TZ[0]) < 1e-5 and abs(TZ[1]) < 1e-5:
                        # print("fix")
                        TZ = np.array([0, 0, -1])
                        TX = np.array([1, 0, 0])
                    else:
                        t = math.sqrt(TZ[0] ** 2 + TZ[2] ** 2)
                        TX = np.array([-TZ[2] / t, 0, TZ[0] / t])

                    TY = np.cross(TZ, TX)
                    print("xyz ", TX, TY, TZ)
                    M = np.array([TX, TY, TZ]).T

                    R = SSTR.from_matrix(M)
                    # rotvet = R.as_rotvec()
                    # print("vet ", rotvet)
                    # Rx, Ry, Rz = rotvet[0], rotvet[1], rotvet[2]
                    euler = R.as_euler('XYZ', degrees=False)
                    Rx, Ry, Rz = euler[0], euler[1], euler[2]

                    robot_pos -= self.track_distance / 1000 * robot_normal

                    # print("robot_normal", Rx, Ry, Rz)
                    if USEROBOT and rxmin <= robot_pos[0] <= rxmax and rymin <= robot_pos[1] <= rymax and rzmin <= \
                            robot_pos[
                                2] <= rzmax:
                        pos = np.array([robot_pos[0], robot_pos[1], robot_pos[2], Rx, Ry, Rz])
                        print("move ", pos)
                        self.model.TCPServer.addPoint(pos)

            if USEHAND:
                hand_pos, hand_normal, color_img = self.model.get_handpos(color_img)
                # print("hand_pos:", hand_pos)
                self.view.showCameraPos(hand_pos)
                if hand_pos is not None:  # and self.model.robot_can_move:
                    robot_pos = np.dot(self.R, hand_pos) + self.T.reshape(1, 3)
                    # print("robot_pos:", robot_pos)
                    robot_pos = robot_pos[0] / 1000

                    robot_normal = np.dot(self.R, hand_normal)
                    robot_normal /= np.linalg.norm(robot_normal)
                    # print(robot_normal)
                    # if robot_normal[2] > 0:
                    #     # 朝下
                    #     robot_normal = -robot_normal

                    # theta = math.pi * 60 / 180  # 若 normal 倾斜超过 theta, 矫正到 theta 内
                    # robot_z_vector = np.array([0, 0, -1])
                    # if robot_z_vector.dot(robot_normal) < math.cos(theta):
                    #     nx, ny = robot_normal[0], robot_normal[1]
                    #     al = math.sin(theta)
                    #     l = math.sqrt(nx * nx + ny * ny)
                    #     x, y = al / l * nx, al / l * ny
                    #     a = np.array([x, y, -math.cos(theta)])
                    #     robot_normal = a / np.linalg.norm(a)

                    # 末端旋转矩阵
                    # print("rb ",robot_normal)
                    TZ = robot_normal
                    if abs(TZ[0]) < 1e-5 and abs(TZ[1]) < 1e-5:
                        TZ = np.array([0, 0, -1])
                        TX = np.array([1, 0, 0])
                    else:
                        t = math.sqrt(TZ[0] ** 2 + TZ[2] ** 2)
                        TX = np.array([-TZ[2] / t, 0, TZ[0] / t])  # tx dot tz=0
                    # print("tz", TZ)
                    TY = np.cross(TZ, TX)

                    # z水平的情况
                    # TY = robot_normal
                    # if TY[0] < 1e-4 and TY[1] < 1e-4:
                    #     TY = np.array([0, 0, -1])
                    #     TX = np.array([-1, 0, 0])
                    # else:
                    #     t = math.sqrt(TY[0] ** 2 + TY[2] ** 2)
                    #     TX = np.array([TY[2] / t, 0, -TY[0] / t])
                    # TZ = np.cross(TX, TY)

                    M = np.array([TX, TY, TZ]).T
                    # print(M)
                    R = SSTR.from_matrix(M)
                    # rotvet = R.as_rotvec()
                    euler = R.as_euler('XYZ', degrees=False)
                    Rx, Ry, Rz = euler[0], euler[1], euler[2]

                    # Rx, Ry, Rz = rotvet[0], rotvet[1], rotvet[2]
                    # print(Rx, Ry, Rz)
                    robot_pos -= 0.100 * robot_normal
                    self.view.showRobotPos(robot_pos)
                    # print("robot_normal", robot_normal)
                    print("robot_pos", robot_pos)
                    if USEROBOT and rxmin <= robot_pos[0] <= rxmax and rymin <= robot_pos[1] <= rymax and rzmin <= \
                            robot_pos[2] <= rzmax:
                        pos = np.array([robot_pos[0], robot_pos[1], robot_pos[2], Rx, Ry, Rz])
                        print("move ", pos)
                        self.model.TCPServer.addPoint(pos)

            # 点击鼠标刷新跟踪位置
            if self.view.style.clicked:
                self.view.showPosNormal()
                self.view.style.clicked = False

            height, width, channel = color_img.shape
            color_img = cv2.resize(color_img, (width // 2, height // 2))  # 图片压缩处理
            img = QImage(color_img.data, width // 2, height // 2, width // 2 * channel, QImage.Format_RGB888)
            self.view.showrgbView(img)

            # 二维脑分区
            hbimg = cv2.imread('./data/hhbbb.jpg')
            # hbimg = cv2.resize(hbimg, (170, 140))
            height, width, channel = hbimg.shape
            if self.view.style.track_position is not None:
                # 计算点击位置在二维图的分布
                px = -int(1000 * self.view.style.track_position[0])
                pz = -int(1000 * self.view.style.track_position[2])
                wc, hc, wr, hr = height, width, 175, 160
                dx, dy = width / 2 - 25, height / 2
                cx, cy = -(pz) * wc / wr + dx, (px) * hc / hr + dy
                cx, cy = int(cx), int(cy)
                # print(cx, cy)
                cv2.line(hbimg, [cx, 0], [cx, height - 1], (0, 255, 255), 1)
                cv2.line(hbimg, [0, cy], [width - 1, cy], (255, 255, 0), 1)
                # cv2.putText(hbimg, str(pz), (cx, height-1), cv2.FONT_HERSHEY_PLAIN,
                #             0.8, (255, 255, 255), thickness=1)
                # cv2.putText(hbimg, str(px), (0, cy-1), cv2.FONT_HERSHEY_PLAIN,
                #             0.8, (255, 255, 255), thickness=1)
                cv2.circle(hbimg, [cx, cy], 3, (0, 0, 255), -1)

            hbimg = QImage(hbimg.data, width, height, width * channel, QImage.Format_BGR888)
            self.view.showdepthView(hbimg)

            if self.model.robot_can_move:
                self.view.showDebugInfo("robot运行")
            else:
                self.view.showDebugInfo("robot停止")


# vtk 处理鼠标点击事件
class myInteractorStyle(vtk.vtkInteractorStyleTrackballCamera):
    def __init__(self, ren):
        # super(myInteractorStyle, self).__init__()
        self.last_picked_actor = None
        self.last_picked_eeactor = None
        self.AddObserver("LeftButtonPressEvent", self.leftButtonPressEvent)
        self.AddObserver("RightButtonPressEvent", self.rightButtonPressEvent)
        self.AddObserver("RightButtonReleaseEvent", self.rightButtonReleaseEvent)
        self.track_position = None  # 鼠标点击跟踪位置
        self.track_normal = None
        self.clicked = False
        self.ren = ren

    def leftButtonPressEvent(self, obj, event):
        inter1 = self.GetInteractor()
        click_pos = inter1.GetEventPosition()
        picker = vtk.vtkCellPicker()
        picker.Pick(click_pos[0], click_pos[1], 0, self.ren)
        pos = picker.GetPickPosition()
        normal = picker.GetPickNormal()
        self.track_position = np.array(pos)
        self.track_normal = -np.array(normal)
        print('pos:', self.track_position)
        print('normal', self.track_normal)

        if self.last_picked_actor is not None:
            self.ren.RemoveActor(self.last_picked_actor)
        if self.last_picked_eeactor is not None:
            self.ren.RemoveActor(self.last_picked_eeactor)

        sphereSource = vtk.vtkSphereSource()
        sphereSource.SetCenter(pos[0], pos[1], pos[2])
        sphereSource.SetRadius(0.005)
        sphereMapper = vtk.vtkPolyDataMapper()
        sphereMapper.SetInputConnection(sphereSource.GetOutputPort())
        sphereActor = vtk.vtkActor()
        sphereActor.SetMapper(sphereMapper)
        sphereActor.GetProperty().SetColor(255, 0, 0)

        pos = self.track_position + 50 * self.track_normal / 1000
        eesphereSource = vtk.vtkSphereSource()
        eesphereSource.SetCenter(pos[0], pos[1], pos[2])
        eesphereSource.SetRadius(0.005)
        eesphereMapper = vtk.vtkPolyDataMapper()
        eesphereMapper.SetInputConnection(eesphereSource.GetOutputPort())
        eesphereActor = vtk.vtkActor()
        eesphereActor.SetMapper(eesphereMapper)
        eesphereActor.GetProperty().SetColor(0, 255, 0)

        self.last_picked_actor = sphereActor
        if self.last_picked_actor is not None:
            self.ren.AddActor(sphereActor)
        self.last_picked_eeactor = eesphereActor
        if self.last_picked_eeactor is not None:
            self.ren.AddActor(eesphereActor)
        self.clicked = True
        # 主要是为了刷新标记点
        self.OnLeftButtonDown()
        self.OnLeftButtonUp()
        # vtkWidget.GetRenderWindow().Render()

    def rightButtonPressEvent(self, *arg):
        self.OnLeftButtonDown()

    def rightButtonReleaseEvent(self, *args):
        self.OnLeftButtonUp()


def GetCenter(poly_data):
    points: vtkmodules.vtkCommonCore.vtkPoints = poly_data.GetPoints()
    ps = [points.GetPoint(i) for i in range(poly_data.GetNumberOfPoints())]
    xyz = tuple(sum(v) / poly_data.GetNumberOfPoints() for v in zip(*ps))
    return xyz


def GetSize(poly_data):
    points: vtkmodules.vtkCommonCore.vtkPoints = poly_data.GetPoints()
    ps = [points.GetPoint(i) for i in range(poly_data.GetNumberOfPoints())]
    xyz_min = [min(v) for v in zip(*ps)]
    xyz_max = [max(v) for v in zip(*ps)]
    size = tuple(xyz_max[i] - xyz_min[i] for i in range(3))
    return size


def GetScaleFactor():
    pass


class MainWindow(QMainWindow):
    def __init__(self):

        super(MainWindow, self).__init__()
        self.ui = Ui_MainWindow()
        self.ui.setupUi(self)
        self.isclosed = False

        # ----> SET WINDOW TITLE AND ICON
        self.setWindowTitle("hello world")

        self.ui.lbvideo.setMaximumSize(550, 440)
        self.ui.lbvideo.setPixmap(QPixmap(550, 440))
        self.ui.lbvideo.setScaledContents(True)

        self.ui.lbdepth.setMaximumSize(550, 440)
        self.ui.lbdepth.setPixmap(QPixmap(550, 440))
        self.ui.lbdepth.setScaledContents(True)

        # global ren, vtkWidget

        self.vtkWidget = QVTKRenderWindowInteractor(self.ui.lbmodel)  # 创建3D模型加载窗口
        self.vtkWidget.setWindowOpacity(0)  # 设置透明度
        self.vtkWidget.resize(550, 440)  # 设置3D模型大小
        # vtkWidget = QVTKRenderWindowInteractor(self.ui.lbmodel)  # 创建3D模型加载窗口
        # vtkWidget.setWindowOpacity(0)  # 设置透明度
        # vtkWidget.resize(550, 540)  # 设置3D模型大小

        # self.filename = "./output/hello_flame.obj"

        self.filename = "./output/s2.obj"

        # create the obj reader and read a data file
        self.reader = vtkOBJReader()
        self.reader.SetFileName(self.filename)
        self.reader.Update()

        sx, sy, sz = GetSize(self.reader.GetOutput())
        sx, sy, sz = 0.2120719999074936 / sx, 0.3271860033273697 / sz, 0.22588500380516052 / sy
        cx, cy, cz = GetCenter(self.reader.GetOutput())

        # 模型变换
        self.transform = vtk.vtkTransform()

        self.transform.Scale(sx, sy / 2, sz)
        self.transform.RotateX(90)
        self.transform.Translate(-cx, -cy, -cz)

        # 新建变换的 filter
        self.transformFilter = vtk.vtkTransformPolyDataFilter()
        # 将变换 filter 输入设置为点数据模型
        self.transformFilter.SetInputConnection(self.reader.GetOutputPort())
        # 设置变换过程
        self.transformFilter.SetTransform(self.transform)
        self.transformFilter.Update()
        # Mappers: 映射器对象主要作用是将可视化模型生成的数据转换到图形模型进行绘制，或者以磁盘文件的形式进行输出。
        self.mapper = vtk.vtkPolyDataMapper()
        self.mapper.SetInputConnection(self.transformFilter.GetOutputPort())
        # 创建一个actor
        self.actor = vtk.vtkActor()
        self.actor.SetMapper(self.mapper)
        # lighting
        self.light1 = vtk.vtkLight()
        self.light1.SetIntensity(.4)
        self.light1.SetPosition(10, 10, -10)
        self.light1.SetDiffuseColor(1, 1, 1)
        self.light2 = vtk.vtkLight()
        self.light2.SetIntensity(.4)
        self.light2.SetPosition(-10, 10, -10)
        self.light2.SetDiffuseColor(1, 1, 1)
        self.light3 = vtk.vtkLight()
        self.light3.SetIntensity(.4)
        self.light3.SetPosition(0, -10, 10)
        self.light3.SetDiffuseColor(1, 1, 1)
        self.light4 = vtk.vtkLight()
        self.light4.SetIntensity(.4)
        self.light4.SetPosition(0, 10, 10)
        self.light4.SetDiffuseColor(1, 1, 1)
        # Assign actor to the renderer
        # Create a rendering window and renderer
        self.ren = vtk.vtkRenderer()
        self.ren.AddActor(self.actor)
        self.ren.AddLight(self.light1)
        self.ren.AddLight(self.light2)
        self.ren.AddLight(self.light3)
        self.ren.AddLight(self.light4)
        self.ren.SetBackground(0.1, 0.1, 0.1)  # Background color
        # set a camera
        self.camera = vtk.vtkCamera()
        # 以世界坐标设置摄像机的位置
        self.camera.SetPosition(0, 0.5, -0.8)
        self.ren.SetActiveCamera(self.camera)
        # create coordinate axes in thee render window
        self.axes = vtk.vtkAxesActor()
        self.axes.SetPosition(0, 0, 0)
        self.axes.SetTotalLength(0.1, 0.1, 0.1)
        self.axes.SetShaftType(0)
        self.axes.SetCylinderRadius(0.002)
        self.axes.SetAxisLabels(0)
        self.ren.AddActor(self.axes)
        # 将 render 加入到renderwindow
        self.vtkWidget.GetRenderWindow().AddRenderer(self.ren)
        # 获取窗口交互工具
        self.iren = self.vtkWidget.GetRenderWindow().GetInteractor()
        # vtkWidget.GetRenderWindow().AddRenderer(ren)
        # self.iren = vtkWidget.GetRenderWindow().GetInteractor()
        # self.iren.Initialize()
        self.style = myInteractorStyle(self.ren)
        self.iren.SetInteractorStyle(self.style)
        # renWin.Render()
        self.iren.Start()

        #############################################################

        self.ms = MySignal()
        self.ms.str_msg.connect(self._updateLabel)
        # self.ms.ndarray_msg.connect(self.updateCoordinate)
        # self.ms.int_msg.connect(self.updateBoxValue)
        self.ms.progressbar_int_msg.connect(self._updateProgressBar)
        self.ms.imgviewer_msg.connect(self._updateView)
        self.ms.model_msg.connect(self._updateModel)

    def closeEvent(self, a0: QtGui.QCloseEvent) -> None:
        self.isclosed = True

    def _updateLabel(self, lb, info):
        lb.setText(info)

    def _updateProgressBar(self, pb, val):
        pb.setValue(val)

    def _updateView(self, lb, data):
        pix_img = QPixmap.fromImage(data)
        # pix_img = pix_img.scaled(550, 540, QtCore.Qt.KeepAspectRatio)
        lb.setPixmap(pix_img)
        lb.setScaledContents(True)

    def reload3DModel(self):
        self.ms.model_msg.emit(self.vtkWidget, 0)

    def _updateModel(self):
        self.ren.RemoveAllViewProps()
        self.vtkWidget.GetRenderWindow().Render()
        # filename = self.filename
        filename = './output/fit_lmk3d_result.obj'
        reader = vtk.vtkOBJReader()
        reader.SetFileName(filename)
        mapper = vtk.vtkPolyDataMapper()
        mapper.SetInputConnection(reader.GetOutputPort())
        actor = vtk.vtkActor()
        actor.SetMapper(mapper)
        self.ren.AddActor(actor)
        self.vtkWidget.GetRenderWindow().Render()

    def showrgbView(self, data):
        self.ms.imgviewer_msg.emit(self.ui.lbvideo, data)

    def showdepthView(self, data):
        self.ms.imgviewer_msg.emit(self.ui.lbdepth, data)

    def showDebugInfo(self, info):
        self.ms.str_msg.emit(self.ui.lb12, str(info))

    def showCameraPos(self, camera_pos):
        if camera_pos is not None:
            msg = str(round(camera_pos[0], 0)) + ", " \
                  + str(round(camera_pos[1], 0)) + ", " \
                  + str(round(camera_pos[2], 0))
        else:
            msg = " - , - , - "
        self.ms.str_msg.emit(self.ui.lb22, msg)

    def showRobotPos(self, robot_pos):
        if robot_pos is not None:
            msg = str(round(1000 * robot_pos[0], 0)) + ", " \
                  + str(round(1000 * robot_pos[1], 0)) + ", " \
                  + str(round(1000 * robot_pos[2], 0))
        else:
            msg = " - , - , - "
        self.ms.str_msg.emit(self.ui.lb32, msg)

    def showPosNormal(self):
        head_pos, head_normal = self.style.track_position, self.style.track_normal
        head_pos = 1000 * head_pos
        self.ms.str_msg.emit(self.ui.pos_x, str(round(head_pos[0], 1)))
        self.ms.str_msg.emit(self.ui.pos_y, str(round(head_pos[1], 1)))
        self.ms.str_msg.emit(self.ui.pos_z, str(round(head_pos[2], 1)))
        self.ms.str_msg.emit(self.ui.nor_x, str(round(head_normal[0], 3)))
        self.ms.str_msg.emit(self.ui.nor_y, str(round(head_normal[1], 3)))
        self.ms.str_msg.emit(self.ui.nor_z, str(round(head_normal[2], 3)))


if __name__ == "__main__":
    app = QApplication(sys.argv)
    logi = Controller()
    logi.start()
    sys.exit(app.exec_())
