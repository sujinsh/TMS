# 加载obj模型
# !/usr/bin/env python
import cv2
# import pyk4a
# from helpers import colorize
# from pyk4a import Config, PyK4A
import vtk
# from vtk import *
import mediapipe as mp
from vtk.qt.QVTKRenderWindowInteractor import QVTKRenderWindowInteractor
import sys
# IMPORTING ALL THE NECESSERY PYSIDE2 MODULES FOR OUR APPLICATION.
from PyQt5 import QtCore, QtGui, QtWidgets
from PyQt5.QtCore import *
from PyQt5.QtGui import *
from PyQt5.QtWidgets import *

from ui_tmsvison import Ui_MainWindow  # MAINWINDOW CODE GENERATED BY THE QT DESIGNER AND pyside2-uic.
from ttestkinect import Camera
from ttestur3 import UR3
# from ttestTVGrobot import TVGserial  # 机器人串口控制
from Filters import MeanFilter, WeightedFilter  # 滤波器
import math
from scipy.spatial.transform import Rotation as SSTR
# import matplotlib

# matplotlib.use("Qt5Agg")  # 声明使用QT5
# from matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas
# from matplotlib.figure import Figure
from matplotlib import pyplot
import numpy as np
# from scipy.signal import butter, lfilter
import threading
import time
import chumpy as ch
from os.path import join

from smpl_webuser.serialization import load_model
from fitting.landmarks import load_embedding, landmark_error_3d
from fitting.util import load_binary_pickle, write_simple_obj, safe_mkdir, get_unit_factor

USEROBOT = False
USEHAND = False
USEFACE = True

rxmin, rxmax = -0.240, 0.240
rymin, rymax = -0.500, -0.100
rzmin, rzmax = -0.050, 0.400

# 水平和垂直翻转后的roi图像区域
cxmin, cxmax = 380, 840
cymin, cymax = 100, 700

# face_ids = [234, 127, 162, 21, 54, 103, 67, 109, 10, 338, 297, 332, 284, 251, 389, 93, 132, 58, 138, 172, 136, 150, 149, 176, 148, 152, 377, 400, 378, 379, 365, 397, 288, 361, 323, 454, 356]
# 面向患者视角的左右眼， 图像的左右
# left_eye_ids = [229, 230, 231, 224, 223, 222]
# right_eye_ids = [444, 443, 442, 449, 450, 451]
# nose_ids = [4,5,45,275]
# mouth_ids = [0,18, 182, 406]
#
# mp_left_eyes    = [229, 230, 231, 224, 223, 222]
# mp_right_eyes   = [444, 443, 442, 449, 450, 451]
# mp_noses        = [4, 5, 45, 275]
# mp_mouths       = [0,18, 182, 406]

flame_left_eyes     = [i-1 for i in [23]]
flame_right_eyes    = [i-1 for i in [26]]
flame_noses         = [i-1 for i in [14]]
flame_mouths       = [i - 1 for i in [35]]
#  FLAME 的id 转化到 mediapipe 的id
# flame2mps         = [70, 63, 105, 66, 107, 336, 296, 334, 293, 300, 168, 6, 195, 4, 79, 60, 2, 290, 309, 33, 159, 157, 133, 153, 144, 362, 384, 386, 263, 373, 380, 61, 39, 37, 0, 267, 269, 291, 321, 314, 17, 84, 181, 78, 82, 13, 312, 308, 317, 14, 87]
flame2mps           = [70, 63, 105, 66, 107, 336, 296, 334, 293, 300, 168, 6, 195, 4, 98, 97, 2, 326, 327, 33, 159, 157, 133, 153, 144, 362, 384, 386, 263, 373, 380, 61, 39, 37, 0, 267, 269, 291, 321, 314, 17, 84, 181, 78, 82, 13, 312, 308, 317, 14, 87]
#                     '1   2   3    4    5    6    7    8    9   10   11   12  13  14  15  16 17  18   19   20  21   22   23   24   25   26   27   28   29   30   31  32  33  34  35  36  37   38   39   40   41  42  43  44  45  46  47   48   49   50  51'
base_left_eye_ids = [i - 1 for i in [1, 2, 3, 4, 5, 20, 21, 22, 23, 24, 25]]
base_right_eye_ids = [i - 1 for i in [6, 7, 8, 9, 10, 26, 27, 28, 29, 30, 31]]
base_nose_ids = [i - 1 for i in [11, 12, 13, 14, 15, 16, 17, 18, 19]]
base_mouth_ids = [i - 1 for i in range(32, 52)]

left_parts = [i-1 for i in [1,2,3,4,5, 20,21,22,23,24,25]]
right_parts = [i-1 for i in [10,9,8,7,6, 29,28,27,26,31,30]]

important_ids = [5, 6, 7, 8, 9, 10, 11, 12, 13, 22, 25, 26, 27, 28, 29, 30, 33, 35, 39, 41]


# 信号发送
class MySignal(QObject):
    str_msg = pyqtSignal(QLabel, str)
    ndarray_msg = pyqtSignal(np.ndarray)
    progressbar_int_msg = pyqtSignal(QProgressBar, int)
    int_msg = pyqtSignal(int)
    imgviewer_msg = pyqtSignal(QLabel, QImage)
    model_msg = pyqtSignal(QWidget, int)


class Model(threading.Thread):
    def __init__(self):
        threading.Thread.__init__(self)
        self.ca = Camera()
        self.ca.start()
        # USE UR3Robot
        if USEROBOT:
            self.robot = UR3("192.168.1.3", track_threshold=0.05, payload=0.0)
            self.robot.start()
            time.sleep(0.1)
            print("启动机器人")
        self.robot_can_move = False
        # self.robot_init_pos = np.array([283.0, 0.0, 210.0, 0.0, 180.0, 0.0])
        self.filter_headx = MeanFilter(20)
        self.filter_nose = MeanFilter(20)
        self.filter_test = MeanFilter(10)
        # self.filter.add(self.robot_init_pos)
        self.Flag = True

        # mp config
        self.mp_drawing = mp.solutions.drawing_utils
        self.mp_face_mesh = mp.solutions.face_mesh
        self.drawing_spec = self.mp_drawing.DrawingSpec(thickness=1, circle_radius=1)

        # flame fitting model
        self.lmk = None  # 人脸标记，用于拟合obj模型
        self.flag_fitting_face = False
        self.flag_fitting_done = False
        self.base_flame_lmks = np.load("./data/scan_lmks.npy")

        model_path = './models/generic_model.pkl'  # change to 'female_model.pkl' or 'male_model.pkl', if gender is known
        self.model = load_model(
            model_path)  # the loaded model object is a 'chumpy' object, check https://github.com/mattloper/chumpy for details
        print("loaded model from:", model_path)

        # landmark embedding
        lmk_emb_path = './models/flame_static_embedding_unix.pkl'
        self.lmk_face_idx, self.lmk_b_coords = load_embedding(lmk_emb_path)
        print("loaded lmk embedding")

        # weights
        self.weights = {}
        # landmark term
        self.weights['lmk'] = 1.0
        # shape regularizer (weight higher to regularize face shape more towards the mean)
        self.weights['shape'] = 1e-3
        # expression regularizer (weight higher to regularize facial expression more towards the mean)
        self.weights['expr'] = 1e-3
        # regularization of head rotation around the neck and jaw opening (weight higher for more regularization)
        self.weights['pose'] = 1e-2

        # optimization options
        import scipy.sparse as sp
        self.opt_options = {}
        self.opt_options['disp'] = 1
        self.opt_options['delta_0'] = 0.1
        self.opt_options['e_3'] = 1e-4
        self.opt_options['maxiter'] = 2000
        sparse_solver = lambda A, x: sp.linalg.cg(A, x, maxiter=self.opt_options['maxiter'])[0]
        self.opt_options['sparse_solver'] = sparse_solver

        if USEHAND:
            self.mpHands = mp.solutions.hands
            self.hands = self.mpHands.Hands()  # 设置参数，详见 hands.py 中的 __init__
            self.mpDraw = mp.solutions.drawing_utils  # 将检测出的手上的标记点连接起来

    def robot_into_follow(self):
        if USEROBOT:
            self.robot_can_move = True
            self.robot.canrun = True

    def robot_exit_follow(self):
        if USEROBOT:
            self.robot_can_move = False
            self.robot.canrun = False

    def stop(self):
        self.Flag = False
        self.ca.stop_kinect()
        if USEROBOT:
            self.robot.stop()


    # 脸部识别主程序
    def face_mark(self, image):
        scale = 1  # 放大roi区域识别人脸
        image_roi = image[cymin:cymax, cxmin:cxmax, :]
        h, w, c = image_roi.shape
        image_roi = cv2.resize(image_roi, (w*scale, h*scale))
        with self.mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5) as face_mesh:
            # image.flags.writeable = False
            results = face_mesh.process(image_roi)
            # image.flags.writeable = True
            if not results.multi_face_landmarks:
                return None, None, image
            for face_landmarks in results.multi_face_landmarks:
                h, w, c = image_roi.shape
                h /= scale
                w /= scale
                points = [np.array([0,0,0]) for i in range(len(flame2mps))]  # flame的51个点在相机下的坐标
                err = False
                for i in range(len(flame2mps)):
                    lm = face_landmarks.landmark[flame2mps[i]]
                    cx, cy = int(lm.x * w) + cxmin, int(lm.y * h) + cymin
                    if 15<=i+1<=19:
                        cy -= 2  # 鼻子周围的点下移
                    cv2.circle(image_roi, [int(lm.x * w * scale), int(lm.y * h * scale)], 1 * scale, (255, 0, 0), 2*scale)
                    p = self.ca.color2point([cx, cy], vflip=True, hflip=True)
                    points[i] = p
                    if i in important_ids:
                        if np.all(p == 0) or np.any(p > 1100):
                            err = True
                image_roi = cv2.resize(image_roi, (int(w), int(h)))
                self.mp_drawing.draw_landmarks(
                    image=image_roi,
                    landmark_list=face_landmarks,
                    connections=self.mp_face_mesh.FACE_CONNECTIONS,
                    landmark_drawing_spec=self.drawing_spec,
                    connection_drawing_spec=self.drawing_spec)
                image[cymin:cymax, cxmin:cxmax, :] = image_roi

                if err:
                    return None, None, image

                left_eye_pos = np.array([0.0, 0.0, 0.0])
                right_eye_pos = np.array([0.0, 0.0, 0.0])
                nose_pos = np.array([0.0, 0.0, 0.0])
                mouth_pos = np.array([0.0, 0.0, 0.0])

                for id in flame_left_eyes:
                    left_eye_pos += points[id]
                left_eye_pos /= len(flame_left_eyes)

                for id in flame_right_eyes:
                    right_eye_pos += points[id]
                right_eye_pos /= len(flame_right_eyes)

                for id in flame_noses:
                    nose_pos += points[id]
                nose_pos /= len(flame_noses)
                self.filter_nose.add(nose_pos)
                nose_pos = self.filter_nose.get()

                for id in flame_mouths:
                    mouth_pos += points[id]
                mouth_pos /= len(flame_mouths)

                # 求旋转平移矩阵
                # print(right_eye_pos, left_eye_pos)
                headX = right_eye_pos - left_eye_pos
                self.filter_headx.add(headX)
                headX = self.filter_headx.get()

                headX = headX / np.linalg.norm(headX)

                vet36_26 = points[26-1] - points[36-1]
                vet34_23 = points[23-1] - points[34-1]
                headY = vet36_26+vet34_23
                headY = headY - np.dot(headY, headX)*headX
                headY /= np.linalg.norm(headY)

                headZ = np.cross(headX, headY)
                headZ = headZ / np.linalg.norm(headZ)

                T_head2ca = nose_pos + 0*headX + 20*headY - 90*headZ
                # print("x y z:", headX, headY, headZ)
                R_head2ca = np.vstack([headX, headY, headZ]).T
                R_ca2head = np.linalg.inv(R_head2ca)
                T_ca2head = -np.dot(R_ca2head, T_head2ca)
                # print("ch: ", T_ca2head,"hc ", T_head2ca)
                id = 11  # 测试id
                p = points[id-1]
                # print(p)
                # print("左右眼鼻口：", left_eye_pos, right_eye_pos, nose_pos, mouth_pos)
                p = np.dot(R_ca2head, p) + T_ca2head
                print(p)

                if abs(p[0]) > 10:
                    print("误差大")
                    return None, None, image
                else:
                    # 使用对称拟合 加亿点修正
                    lmk = []
                    for i in range(len(flame2mps)):
                        p = points[i]
                        p = np.dot(R_ca2head, p) + T_ca2head
                        # print(i + 1, ":", p)
                        lmk.append(p/1000)

                    for i in range(len(left_parts)):
                        leftid, rightid = left_parts[i], right_parts[i]
                        # x对称，y相等 z相等
                        lmk[leftid][0] = -lmk[rightid][0]
                        lmk[leftid][1] = lmk[rightid][1]
                        lmk[leftid][2] = lmk[rightid][2]

                    self.lmk = np.array(lmk)
                    self.lmk[34-1][2] = self.lmk[36-1][2]
                    self.lmk[42-1][2] = self.lmk[40-1][2]

                    lvet = self.lmk[34-1] / 2 + self.lmk[42-1]/2
                    rvet = self.lmk[36-1] / 2 + self.lmk[40-1]/2

                    self.lmk[35-1] = self.lmk[34-1]/2 + self.lmk[36-1]/2
                    self.lmk[41 - 1] = self.lmk[42 - 1] / 2 + self.lmk[40 - 1] / 2

                    self.lmk[32 - 1] = lvet + np.array([-15, 0, -3]) / 1000
                    self.lmk[44 - 1] = lvet + np.array([-10, 0, 0]) / 1000
                    self.lmk[45 - 1] = lvet + np.array([0, 0, 0]) / 1000
                    self.lmk[51 - 1] = lvet + np.array([0, 0, 0]) / 1000

                    self.lmk[38 - 1] = rvet + np.array([15, 0, -3]) / 1000
                    self.lmk[48 - 1] = rvet + np.array([10, 0, 0]) / 1000
                    self.lmk[47 - 1] = rvet + np.array([0, 0, 0]) / 1000
                    self.lmk[49 - 1] = rvet + np.array([0, 0, 0]) / 1000

                    self.lmk[33-1] = self.lmk[32-1]/2 + self.lmk[34-1]/2
                    self.lmk[43-1] = self.lmk[32-1]/2 + self.lmk[42-1]/2
                    self.lmk[37-1] = self.lmk[38-1]/2 + self.lmk[36-1]/2
                    self.lmk[39-1] = self.lmk[38-1]/2 + self.lmk[40-1]/2

                    self.lmk[46-1] = rvet /2 +lvet/2
                    self.lmk[50-1] = rvet /2 +lvet/2

                    self.lmk[17-1] = self.lmk[14-1]*3/5 + self.lmk[35-1]*2/5 + np.array([0,0,-6])/1000
                    self.lmk[16-1] = self.lmk[17-1] + np.array([-5, 2, -1])/1000
                    self.lmk[15-1] = self.lmk[17-1] + np.array([-11, 1, -2])/1000
                    self.lmk[18 - 1] = self.lmk[17 - 1] + np.array([5, 2, -1]) / 1000
                    self.lmk[19 - 1] = self.lmk[17 - 1] + np.array([11, 1, -2]) / 1000
                    # 使用标准点。。。
                    # left_eye = np.dot(R_ca2head,left_eye_pos) + T_ca2head
                    # left_eye_offset = left_eye/1000 - self.base_flame_lmks[23-1]
                    #
                    # right_eye = np.dot(R_ca2head, right_eye_pos) + T_ca2head
                    # right_eye_offset = right_eye/1000 - self.base_flame_lmks[26-1]
                    #
                    # nose = np.dot(R_ca2head, nose_pos) + T_ca2head
                    # nose_offset = nose/1000 - self.base_flame_lmks[14-1]
                    #
                    # mouth = np.dot(R_ca2head, mouth_pos) + T_ca2head
                    # mouth_offset = mouth/1000 - self.base_flame_lmks[35-1]
                    #
                    # self.lmk = self.base_flame_lmks + np.array([0,0,0])
                    # for id in base_left_eye_ids:
                    #     self.lmk[id] += left_eye_offset
                    # for id in base_right_eye_ids:
                    #     self.lmk[id] += right_eye_offset
                    # for id in base_nose_ids:
                    #     self.lmk[id] += nose_offset
                    # for id in base_mouth_ids:
                    #     self.lmk[id] += mouth_offset

                    # print(1000*self.lmk)

                return R_head2ca, T_head2ca, image

            return None, None, image

    def fit_lmk3d(self,
                  lmk_3d,  # input landmark 3d
                  model,  # model
                  lmk_face_idx, lmk_b_coords,  # landmark embedding
                  weights,  # weights for the objectives
                  shape_num=300, expr_num=50, opt_options=None):

        """ function: fit FLAME model to 3D landmarks

        input:
            lmk_3d: input landmark 3D, in shape (N,3)
            model: FLAME face model
            lmk_face_idx, lmk_b_coords: landmark embedding, in face indices and barycentric coordinates
            weights: weights for each objective
            shape_num, expr_num: numbers of shape and expression compoenents used
            opt_options: optimizaton options
        output:
            model.r: fitted result vertices
            model.f: fitted result triangulations (fixed in this code)
            parms: fitted model parameters

        """
        # variables
        pose_idx = np.union1d(np.arange(3), np.arange(6, 9))  # global rotation and jaw rotation
        shape_idx = np.arange(0, min(300, shape_num))  # valid shape component range in "betas": 0-299
        expr_idx = np.arange(300, 300 + min(100, expr_num))  # valid expression component range in "betas": 300-399
        used_idx = np.union1d(shape_idx, expr_idx)
        model.betas[:] = np.random.rand(model.betas.size) * 0.0  # initialized to zero
        model.pose[:] = np.random.rand(model.pose.size) * 0.0  # initialized to zero
        free_variables = [model.trans, model.pose[pose_idx], model.betas[used_idx]]

        # weights
        print("fit_lmk3d(): use the following weights:")
        for kk in weights.keys():
            print("fit_lmk3d(): weights['%s'] = %f" % (kk, weights[kk]))

        # objectives
        # lmk
        lmk_err = landmark_error_3d(mesh_verts=model,
                                    mesh_faces=model.f,
                                    lmk_3d=lmk_3d,
                                    lmk_face_idx=lmk_face_idx,
                                    lmk_b_coords=lmk_b_coords,
                                    weight=weights['lmk'])
        # regularizer
        shape_err = weights['shape'] * model.betas[shape_idx]
        expr_err = weights['expr'] * model.betas[expr_idx]
        pose_err = weights['pose'] * model.pose[3:]  # exclude global rotation
        objectives = {}
        objectives.update({'lmk': lmk_err, 'shape': shape_err, 'expr': expr_err, 'pose': pose_err})

        # options
        if opt_options is None:
            print("fit_lmk3d(): no 'opt_options' provided, use default settings.")
            import scipy.sparse as sp
            opt_options = {}
            opt_options['disp'] = 1
            opt_options['delta_0'] = 0.1
            opt_options['e_3'] = 1e-4
            opt_options['maxiter'] = 2000
            sparse_solver = lambda A, x: sp.linalg.cg(A, x, maxiter=opt_options['maxiter'])[0]
            opt_options['sparse_solver'] = sparse_solver

        # on_step callback
        def on_step(_):
            pass

        # optimize
        # step 1: rigid alignment
        from time import time
        timer_start = time()
        print("\nstep 1: start rigid fitting...")
        ch.minimize(fun=lmk_err,
                    x0=[model.trans, model.pose[0:3]],
                    method='dogleg',
                    callback=on_step,
                    options=opt_options)
        timer_end = time()
        print("step 1: fitting done, in %f sec\n" % (timer_end - timer_start))

        # step 2: non-rigid alignment
        timer_start = time()
        print("step 2: start non-rigid fitting...")
        # ch.minimize(fun=objectives,
        #             x0=free_variables,
        #             method='dogleg',
        #             callback=on_step,
        #             options=opt_options)
        timer_end = time()
        print("step 2: fitting done, in %f sec\n" % (timer_end - timer_start))

        # return results
        parms = {'trans': model.trans.r, 'pose': model.pose.r, 'betas': model.betas.r}
        return model.r, model.f, parms

    def flame_fitting(self, lmk):
        # input landmarks
        # measurement unit of landmarks ['m', 'cm', 'mm']
        scale_factor = 1
        lmk_3d = scale_factor * lmk
        np.save('C:/Users/10401/Desktop/ttt/data/lmk.npy', lmk)
        # number of shape and expression parameters (we do not recommend using too many parameters for fitting to sparse keypoints)
        shape_num = 100
        expr_num = 50

        # run fitting
        mesh_v, mesh_f, parms = self.fit_lmk3d(lmk_3d=lmk_3d,  # input landmark 3d
                                          model=self.model,  # model
                                          lmk_face_idx=self.lmk_face_idx, lmk_b_coords=self.lmk_b_coords,  # landmark embedding
                                          weights=self.weights,  # weights for the objectives
                                          shape_num=shape_num, expr_num=expr_num, opt_options=self.opt_options)  # options

        # write result
        output_dir = './output'
        output_path = join(output_dir, 'fit_lmk3d_result.obj')
        write_simple_obj(mesh_v=mesh_v, mesh_f=mesh_f, filepath=output_path, verbose=False)

    def get_handpos(self, img):
        """
        输入：rgb_img
        输出：手的位置--相机坐标系，法向量--相机坐标系，手的图像
        """
        # imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        image_roi = img[cymin:cymax, cxmin:cxmax, :]
        results = self.hands.process(image_roi)  # 对输入图像进行处理，探索图像中是否有手
        h, w, c = image_roi.shape
        if results.multi_hand_landmarks:
            for handLms in results.multi_hand_landmarks:  # 捕捉画面中的每一只手
                # 取0, 5, 13 三个关节计算手坐标系
                id = 0
                lm = handLms.landmark[id]
                # for id, lm in enumerate(handLms.landmark):
                cx, cy = int(lm.x * w) + cxmin, int(lm.y * h) + cymin  # 根据比例还原出每一个标记点的像素坐标
                p0 = self.ca.color2point([cx, cy], vflip=True, hflip=True)
                if p0 is None or any(np.isnan(p0)) or any(np.isinf(p0)):
                    continue
                # p0[2] += 0.010  # 0 节点太翘

                id = 5
                lm = handLms.landmark[id]
                # for id, lm in enumerate(handLms.landmark):
                cx, cy = int(lm.x * w) + cxmin, int(lm.y * h) + cymin  # 根据比例还原出每一个标记点的像素坐标
                p5 = self.ca.color2point([cx, cy], vflip=True, hflip=True)
                if p5 is None or any(np.isnan(p5)) or any(np.isinf(p5)):
                    continue

                id = 13
                lm = handLms.landmark[id]
                # for id, lm in enumerate(handLms.landmark):
                cx, cy = int(lm.x * w) + cxmin, int(lm.y * h) + cymin  # 根据比例还原出每一个标记点的像素坐标
                p13 = self.ca.color2point([cx, cy], vflip=True, hflip=True)
                if p13 is None or any(np.isnan(p13)) or any(np.isinf(p13)):
                    continue

                hand_pos = (p0+p5+p13)/3
                hand_nomal = 1e3*np.cross(p5-p0, p13-p0)
                if hand_nomal is None or not any(hand_nomal):
                    continue
                hand_nomal /= np.linalg.norm(hand_nomal)
                # print("hp", hand_pos)
                # print("hn", hand_nomal)
                self.mpDraw.draw_landmarks(image_roi, handLms, self.mpHands.HAND_CONNECTIONS)  # 给画面中的每一只手进行标点、连线的操作
                img[cymin:cymax, cxmin:cxmax, :] = image_roi
                return hand_pos, hand_nomal, img
        return None, None, img

    def run(self):
        time.sleep(1)
        while self.Flag:
            if self.flag_fitting_face and self.lmk is not None:
                print("fitting")
                # time.sleep(10)
                lmk = self.lmk + np.array([0, 0, 0])
                np.save("./data/lmk.npy", lmk)
                print(lmk)
                self.flame_fitting(lmk)

                time.sleep(0.5)
                self.flag_fitting_face = False
                self.flag_fitting_done = True
                print("done")
            time.sleep(0.1)
        print("model stop")


class Controller(threading.Thread):

    def __init__(self):
        threading.Thread.__init__(self)
        self.model = Model()
        self.view = MainWindow()
        # 绑定按钮事件
        self.view.ui.pb1.clicked.connect(self.start_robot)
        self.view.ui.pb2.clicked.connect(self.start_fit)
        # self.view.ui.pb3.clicked.connect(self.openFile)
        self.view.ui.pb4.clicked.connect(self.stop)
        self.view.ui.pushButton_6.clicked.connect(self.readdata)

        self.head_pos = None
        self.head_normal = None
        self.track_distance = 30  # unit: mm

        self.Flag = True
        self.view.show()
        self.R = np.array([[-0.99926241, -0.03301532,  0.01961168],
             [-0.00365721,  0.59020813,  0.80724283],
             [-0.03822635 , 0.8065757,  -0.58989354]])
        self.T = np.array([[14.97876835],
             [-1174.31569749],
             [594.91603348]])

        self.model.start()
        time.sleep(1)

    def start_robot(self):
        if not self.model.robot_can_move:
            if USEROBOT:
                self.model.robot_into_follow()
            self.view.ui.pb1.setText("结束跟踪")
        else:
            if USEROBOT:
                self.model.robot_exit_follow()
            self.view.ui.pb1.setText("启动跟踪")

    def start_fit(self):
        self.model.flag_fitting_face = True

    def stop(self):
        self.Flag = False
        self.view.close()
        self.model.stop()

    def readdata(self):
        posx = self.view.ui.pos_x.text()
        posy = self.view.ui.pos_y.text()
        posz = self.view.ui.pos_z.text()
        norx = self.view.ui.nor_x.text()
        nory = self.view.ui.nor_y.text()
        norz = self.view.ui.nor_z.text()
        dist = self.view.ui.tms2head.text()
        # if "" not in [posx, posy, posz, norx, nory, norz]:
        try:
            self.head_pos = np.array([float(posx), float(posy), float(posz)])
            self.head_normal = np.array([float(norx), float(nory), float(norz)])
            self.track_distance = float(dist)
            if self.track_distance <= 30:
                self.track_distance = 30
            if self.track_distance > 200:
                self.track_distance = 200
            print("跟踪：", self.head_pos, self.head_normal, self.track_distance)
        except:
            print("输入错误")
            self.head_pos = None
            self.head_normal = None
            self.track_distance = 30

    def run(self):

        time.sleep(1.5)

        while self.Flag:
            time.sleep(0.03)
            if self.view.isclosed:
                self.model.stop()
                break

            color_img = self.model.ca.get_img("c")
            if color_img is None or not np.any(color_img):
                continue

            color_img = cv2.cvtColor(color_img[:, :, :3], cv2.COLOR_BGR2RGB)
            color_img = cv2.flip(color_img, -1)
            # flip > 0 水平翻转 ,flip =0 垂直翻转 ,flip< 0水平和垂直翻转

            if USEFACE:
                # R_head2ca, T_head2ca, color_img = self.model.face_mark(color_img)
                R_head2ca, T_head2ca, color_img = self.model.face_mark(color_img)
                if R_head2ca is not None and self.head_pos is not None and self.head_normal is not None:
                    # print("hp, hn ",self.head_pos, self.head_normal)
                    cam_head_pos = np.dot(R_head2ca, self.head_pos) + T_head2ca
                    cam_head_normal = np.dot(R_head2ca, self.head_normal)
                    cam_head_normal /= np.linalg.norm(cam_head_normal)
                    # print("cp, cn ", cam_head_pos, cam_head_normal)
                    self.view.showCameraPos(cam_head_pos)

                    robot_pos = np.dot(self.R, cam_head_pos) + self.T.reshape(1, 3)
                    print("robot_pos ", robot_pos)
                    robot_pos = robot_pos[0] / 1000

                    # print("robot_pos", robot_pos)
                    self.view.showRobotPos(robot_pos)

                    robot_normal = np.matmul(self.R, cam_head_normal)

                    if robot_normal[2] > 0:
                        # 朝下
                        robot_normal = -robot_normal
                    print("robot_normal", robot_normal)
                    # theta = math.pi * 60 / 180  # 若 normal 倾斜超过 theta, 矫正到 theta 内
                    # robot_z_vector = np.array([0, 0, -1])
                    # if robot_z_vector.dot(robot_normal) < math.cos(theta):
                    #     nx, ny = robot_normal[0], robot_normal[1]
                    #     al = math.sin(theta)
                    #     l = math.sqrt(nx * nx + ny * ny)
                    #     x, y = al / l * nx, al / l * ny
                    #     a = np.array([x, y, -math.cos(theta)])
                    #     robot_normal = a / np.linalg.norm(a)

                    # 末端旋转矩阵
                    TZ = robot_normal
                    # print('TZ[0]: ', TZ[0], 'TZ[1]: ', TZ[1])
                    if abs(TZ[0]) < 1e-5 and abs(TZ[1]) < 1e-5:
                        # print("fix")
                        TZ = np.array([0, 0, -1])
                        TX = np.array([-1, 0, 0])
                    else:
                        t = math.sqrt(TZ[0] ** 2 + TZ[2] ** 2)
                        TX = np.array([TZ[2] / t, 0, -TZ[0] / t])

                    TY = np.cross(TZ, TX)
                    print("xyz ",TX, TY, TZ)
                    M = np.array([TX, TY, TZ]).T

                    R = SSTR.from_matrix(M)
                    rotvet = R.as_rotvec()
                    # print("vet ", rotvet)
                    Rx, Ry, Rz = rotvet[0], rotvet[1], rotvet[2]
                    robot_pos -= self.track_distance/1000 * robot_normal

                    # print("robot_normal", Rx, Ry, Rz)
                    if USEROBOT and rxmin <= robot_pos[0] <= rxmax and rymin <= robot_pos[1] <= rymax and rzmin <= robot_pos[
                        2] <= rzmax:
                        pos = np.array([robot_pos[0], robot_pos[1], robot_pos[2], Rx, Ry, Rz])
                        print("move ", pos)
                        self.model.robot.filter.add(pos)

            if USEHAND:
                hand_pos, hand_normal, color_img = self.model.get_handpos(color_img)
                # print("hand_pos:", hand_pos)
                self.view.showCameraPos(hand_pos)
                if hand_pos is not None:# and self.model.robot_can_move:
                    robot_pos = np.dot(self.R, hand_pos) + self.T.reshape(1, 3)
                    # print("robot_pos:", robot_pos)
                    robot_pos = robot_pos[0]/1000

                    robot_normal = np.dot(self.R, hand_normal)
                    robot_normal /= np.linalg.norm(robot_normal)
                    # print(robot_normal)
                    if robot_normal[2] > 0:
                        # 朝下
                        robot_normal = -robot_normal

                    theta = math.pi * 60 / 180  # 若 normal 倾斜超过 theta, 矫正到 theta 内
                    robot_z_vector = np.array([0, 0, -1])
                    if robot_z_vector.dot(robot_normal) < math.cos(theta):
                        nx, ny = robot_normal[0], robot_normal[1]
                        al = math.sin(theta)
                        l = math.sqrt(nx*nx + ny*ny)
                        x, y = al/l*nx, al/l*ny
                        a = np.array([x, y, -math.cos(theta)])
                        robot_normal = a/np.linalg.norm(a)


                    # 末端旋转矩阵
                    TZ = robot_normal
                    if abs(TZ[0]) < 1e-5 and abs(TZ[1]) < 1e-5:
                        TZ = np.array([0, 0, -1])
                        TX = np.array([-1, 0, 0])
                    else:
                        t = math.sqrt(TZ[0] ** 2 + TZ[2] ** 2)
                        TX = np.array([TZ[2] / t, 0, -TZ[0] / t])
                    TY = np.cross(TZ, TX)

                    # z水平
                    # TY = robot_normal
                    # if TY[0] < 1e-4 and TY[1] < 1e-4:
                    #     TY = np.array([0, 0, -1])
                    #     TX = np.array([-1, 0, 0])
                    # else:
                    #     t = math.sqrt(TY[0] ** 2 + TY[2] ** 2)
                    #     TX = np.array([TY[2] / t, 0, -TY[0] / t])
                    # TZ = np.cross(TX, TY)

                    M = np.array([TX, TY, TZ]).T

                    R = SSTR.from_matrix(M)
                    rotvet = R.as_rotvec()

                    Rx, Ry, Rz = rotvet[0], rotvet[1], rotvet[2]
                    # robot_pos[2] += 120
                    robot_pos -= 0.100 * robot_normal
                    self.view.showRobotPos(robot_pos)
                    # print("robot_normal", robot_normal)
                    if USEROBOT and rxmin <= robot_pos[0] <= rxmax and rymin <= robot_pos[1] <= rymax and rzmin <= robot_pos[2] <= rzmax:
                        pos = np.array([robot_pos[0], robot_pos[1], robot_pos[2], Rx, Ry, Rz])
                        print("move ", pos)
                        self.model.robot.filter.add(pos)

            # 点击鼠标刷新跟踪位置
            if self.view.style.clicked:
                self.view.showPosNormal()
                self.view.style.clicked = False

            # --刷新fit好的模型
            if self.model.flag_fitting_done:
                self.view.reload3DModel()
                self.model.flag_fitting_done = False

            height, width, channel = color_img.shape
            color_img = cv2.resize(color_img, (width//2, height//2))
            img = QImage(color_img.data, width//2, height//2, width//2*channel, QImage.Format_RGB888)
            self.view.showrgbView(img)

            # 二维脑分区
            hbimg = cv2.imread('./data/hhbbb.jpg')
            # hbimg = cv2.resize(hbimg, (170, 140))
            height, width, channel = hbimg.shape
            if self.view.style.track_position is not None:
                # 计算点击位置在二维图的分布
                px = int(1000*self.view.style.track_position[0])
                pz = int(1000*self.view.style.track_position[2])
                wc, hc, wr, hr = height, width, 175, 160
                dx, dy = width/2-25, height/2
                cx, cy = -(pz)*wc/wr+dx, (px)*hc/hr+dy
                cx, cy = int(cx), int(cy)
                # print(cx, cy)
                cv2.line(hbimg,[cx,0],[cx,height-1],(0,255,255),1)
                cv2.line(hbimg, [0, cy], [width-1, cy], (255, 255, 0), 1)
                # cv2.putText(hbimg, str(pz), (cx, height-1), cv2.FONT_HERSHEY_PLAIN,
                #             0.8, (255, 255, 255), thickness=1)
                # cv2.putText(hbimg, str(px), (0, cy-1), cv2.FONT_HERSHEY_PLAIN,
                #             0.8, (255, 255, 255), thickness=1)
                cv2.circle(hbimg, [cx, cy], 3, (0, 0, 255), -1)

            hbimg = QImage(hbimg.data, width, height, width * channel, QImage.Format_BGR888)
            self.view.showdepthView(hbimg)

            if self.model.robot_can_move:
                self.view.showDebugInfo("robot运行")
            elif self.model.flag_fitting_face:
                self.view.showDebugInfo("拟合中...")
            else:
                self.view.showDebugInfo("robot停止")

# vtk 处理鼠标点击事件
class myInteractorStyle(vtk.vtkInteractorStyleTrackballCamera):
    def __init__(self, ren):
        # super(myInteractorStyle, self).__init__()
        self.last_picked_actor = None
        self.last_picked_eeactor = None
        self.AddObserver("LeftButtonPressEvent", self.leftButtonPressEvent)
        self.AddObserver("RightButtonPressEvent", self.rightButtonPressEvent)
        self.AddObserver("RightButtonReleaseEvent", self.rightButtonReleaseEvent)
        self.track_position = None  # 鼠标点击跟踪位置
        self.track_normal = None
        self.clicked = False
        self.ren = ren

    def leftButtonPressEvent(self, obj, event):
        inter1 = self.GetInteractor()
        click_pos = inter1.GetEventPosition()
        picker = vtk.vtkCellPicker()
        picker.Pick(click_pos[0], click_pos[1], 0, self.ren)
        pos = picker.GetPickPosition()
        normal = picker.GetPickNormal()
        self.track_position = np.array(pos)
        self.track_normal = np.array(normal)
        print('pos:', self.track_position)
        print('normal', self.track_normal)

        if self.last_picked_actor is not None:
            self.ren.RemoveActor(self.last_picked_actor)
        if self.last_picked_eeactor is not None:
            self.ren.RemoveActor(self.last_picked_eeactor)

        sphereSource = vtk.vtkSphereSource()
        sphereSource.SetCenter(pos[0], pos[1], pos[2])
        sphereSource.SetRadius(0.005)
        sphereMapper = vtk.vtkPolyDataMapper()
        sphereMapper.SetInputConnection(sphereSource.GetOutputPort())
        sphereActor = vtk.vtkActor()
        sphereActor.SetMapper(sphereMapper)
        sphereActor.GetProperty().SetColor(255, 0, 0)

        pos = self.track_position + 50*self.track_normal/1000
        eesphereSource = vtk.vtkSphereSource()
        eesphereSource.SetCenter(pos[0], pos[1], pos[2])
        eesphereSource.SetRadius(0.005)
        eesphereMapper = vtk.vtkPolyDataMapper()
        eesphereMapper.SetInputConnection(eesphereSource.GetOutputPort())
        eesphereActor = vtk.vtkActor()
        eesphereActor.SetMapper(eesphereMapper)
        eesphereActor.GetProperty().SetColor(0, 255, 0)

        self.last_picked_actor = sphereActor
        if self.last_picked_actor is not None:
            self.ren.AddActor(sphereActor)
        self.last_picked_eeactor = eesphereActor
        if self.last_picked_eeactor is not None:
            self.ren.AddActor(eesphereActor)
        self.clicked = True
        # 主要是为了刷新标记点
        self.OnLeftButtonDown()
        self.OnLeftButtonUp()
        # vtkWidget.GetRenderWindow().Render()

    def rightButtonPressEvent(self, *arg):
        self.OnLeftButtonDown()

    def rightButtonReleaseEvent(self, *args):
        self.OnLeftButtonUp()


class MainWindow(QMainWindow):
    def __init__(self):

        super(MainWindow, self).__init__()
        self.ui = Ui_MainWindow()
        self.ui.setupUi(self)
        self.isclosed = False

        # ----> SET WINDOW TITLE AND ICON
        applicationName = "hello world"
        self.setWindowTitle(applicationName)

        self.ui.lbvideo.setMaximumSize(550, 440)
        self.ui.lbvideo.setPixmap(QPixmap(550, 440))
        self.ui.lbvideo.setScaledContents(True)

        self.ui.lbdepth.setMaximumSize(550, 440)
        self.ui.lbdepth.setPixmap(QPixmap(550, 440))
        self.ui.lbdepth.setScaledContents(True)

        # global ren, vtkWidget

        self.vtkWidget = QVTKRenderWindowInteractor(self.ui.lbmodel)  # 创建3D模型加载窗口
        self.vtkWidget.setWindowOpacity(0)  # 设置透明度
        self.vtkWidget.resize(550, 440)  # 设置3D模型大小
        # vtkWidget = QVTKRenderWindowInteractor(self.ui.lbmodel)  # 创建3D模型加载窗口
        # vtkWidget.setWindowOpacity(0)  # 设置透明度
        # vtkWidget.resize(550, 540)  # 设置3D模型大小

        self.filename = "./output/hello_flame.obj"
        self.reader = vtk.vtkOBJReader()
        self.reader.SetFileName(self.filename)
        self.mapper = vtk.vtkPolyDataMapper()
        self.mapper.SetInputConnection(self.reader.GetOutputPort())
        self.actor = vtk.vtkActor()
        self.actor.SetMapper(self.mapper)
        # Assign actor to the renderer
        # Create a rendering window and renderer
        self.ren = vtk.vtkRenderer()
        self.ren.AddActor(self.actor)
        # Enable user interface interactor
        self.vtkWidget.GetRenderWindow().AddRenderer(self.ren)
        self.iren = self.vtkWidget.GetRenderWindow().GetInteractor()
        # vtkWidget.GetRenderWindow().AddRenderer(ren)
        # self.iren = vtkWidget.GetRenderWindow().GetInteractor()
        # self.iren.Initialize()
        self.style = myInteractorStyle(self.ren)
        self.iren.SetInteractorStyle(self.style)
        # renWin.Render()
        self.iren.Start()

        #############################################################

        self.ms = MySignal()
        self.ms.str_msg.connect(self._updateLabel)
        # self.ms.ndarray_msg.connect(self.updateCoordinate)
        # self.ms.int_msg.connect(self.updateBoxValue)
        self.ms.progressbar_int_msg.connect(self._updateProgressBar)
        self.ms.imgviewer_msg.connect(self._updateView)
        self.ms.model_msg.connect(self._updateModel)

    def closeEvent(self, a0: QtGui.QCloseEvent) -> None:
        self.isclosed = True

    def _updateLabel(self, lb, info):
        lb.setText(info)

    def _updateProgressBar(self, pb, val):
        pb.setValue(val)

    def _updateView(self, lb, data):
        pix_img = QPixmap.fromImage(data)
        # pix_img = pix_img.scaled(550, 540, QtCore.Qt.KeepAspectRatio)
        lb.setPixmap(pix_img)
        lb.setScaledContents(True)

    def reload3DModel(self):
        self.ms.model_msg.emit(self.vtkWidget, 0)

    def _updateModel(self):
        self.ren.RemoveAllViewProps()
        self.vtkWidget.GetRenderWindow().Render()
        # filename = self.filename
        filename = './output/fit_lmk3d_result.obj'
        reader = vtk.vtkOBJReader()
        reader.SetFileName(filename)
        mapper = vtk.vtkPolyDataMapper()
        mapper.SetInputConnection(reader.GetOutputPort())
        actor = vtk.vtkActor()
        actor.SetMapper(mapper)
        self.ren.AddActor(actor)
        self.vtkWidget.GetRenderWindow().Render()


    def showrgbView(self, data):
        self.ms.imgviewer_msg.emit(self.ui.lbvideo, data)

    def showdepthView(self, data):
        self.ms.imgviewer_msg.emit(self.ui.lbdepth, data)

    def showDebugInfo(self, info):
        self.ms.str_msg.emit(self.ui.lb12, str(info))

    def showCameraPos(self, camera_pos):
        if camera_pos is not None:
            msg = str(round(camera_pos[0], 0)) + ", " \
                  + str(round(camera_pos[1], 0)) + ", "\
                  + str(round(camera_pos[2], 0))
        else:
            msg = " - , - , - "
        self.ms.str_msg.emit(self.ui.lb22, msg)

    def showRobotPos(self, robot_pos):
        if robot_pos is not None:
            msg = str(round(1000*robot_pos[0], 0)) + ", "\
                  + str(round(1000*robot_pos[1], 0)) + ", "\
                  + str(round(1000*robot_pos[2], 0))
        else:
            msg = " - , - , - "
        self.ms.str_msg.emit(self.ui.lb32, msg)

    def showPosNormal(self):
        head_pos, head_normal = self.style.track_position, self.style.track_normal
        head_pos = 1000*head_pos
        self.ms.str_msg.emit(self.ui.pos_x, str(round(head_pos[0],1)))
        self.ms.str_msg.emit(self.ui.pos_y, str(round(head_pos[1],1)))
        self.ms.str_msg.emit(self.ui.pos_z, str(round(head_pos[2],1)))
        self.ms.str_msg.emit(self.ui.nor_x, str(round(head_normal[0],3)))
        self.ms.str_msg.emit(self.ui.nor_y, str(round(head_normal[1],3)))
        self.ms.str_msg.emit(self.ui.nor_z, str(round(head_normal[2],3)))


if __name__ == "__main__":
    app = QApplication(sys.argv)
    logi = Controller()
    logi.start()
    sys.exit(app.exec_())
