# 加载obj模型
# !/usr/bin/env python
import cv2
# import pyk4a
# from helpers import colorize
# from pyk4a import Config, PyK4A
import vtk
import mediapipe as mp
from vtk.qt.QVTKRenderWindowInteractor import QVTKRenderWindowInteractor
import sys
# IMPORTING ALL THE NECESSERY PYSIDE2 MODULES FOR OUR APPLICATION.
from PyQt5 import QtCore, QtGui, QtWidgets
from PyQt5.QtCore import *
from PyQt5.QtGui import *
from PyQt5.QtWidgets import *

from ui_tmsvison import Ui_MainWindow  # MAINWINDOW CODE GENERATED BY THE QT DESIGNER AND pyside2-uic.
from ttestkinect import Camera
from ttestTVGrobot import TVGserial  # 机器人串口控制
from Filters import MeanFilter, WeightedFilter  # 滤波器
import math
# from scipy.spatial.transform import Rotation as SSTR
# import matplotlib

# matplotlib.use("Qt5Agg")  # 声明使用QT5
# from matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas
# from matplotlib.figure import Figure
# from matplotlib import pyplot
import numpy as np
# from scipy.signal import butter, lfilter
import threading
import time
# import pickle
# import pandas as pd
# from wfdb import processing
# from tensorflow import keras
# from tensorflow.keras.layers import Dense, Input, Dropout, Convolution1D, MaxPool1D, GlobalMaxPool1D, \
#     GlobalAveragePooling1D, concatenate

USEROBOT = True
USEHAND = False
USEFACE = True

rxmin, rxmax = 180, 490
rymin, rymax = -160, 160
rzmin, rzmax = -50, 400

# 水平和垂直翻转后的roi图像区域
cxmin, cxmax = 420, 810
cymin, cymax = 100, 520

face_ids = [234, 127, 162, 21, 54, 103, 67, 109, 10, 338, 297, 332, 284, 251, 389, 93, 132, 58, 138, 172, 136, 150, 149, 176, 148, 152, 377, 400, 378, 379, 365, 397, 288, 361, 323, 454, 356]
# 面向患者视角的左右眼， 图像的左右
left_eye_ids = [229, 230, 231, 224, 223, 222]
right_eye_ids = [444, 443, 442, 449, 450, 451]
nose_ids = [4,5,45,275]
mouth_ids = [0,18, 182, 406]
SPEED = 5

class MySignal(QObject):
    str_msg = pyqtSignal(QLabel, str)
    ndarray_msg = pyqtSignal(np.ndarray)
    progressbar_int_msg = pyqtSignal(QProgressBar, int)
    int_msg = pyqtSignal(int)
    rgbviewer_msg = pyqtSignal(QLabel, QImage)


class Model(threading.Thread):
    def __init__(self):
        threading.Thread.__init__(self)
        self.ca = Camera()
        self.ca.start()
        # if USEROBOT:
        self.robot = TVGserial(com="COM5", baud=115200)
        self.robot.start()
        print("启动机器人")
        self.robot_can_move = False
        self.robot_init_pos = np.array([283.0, 0.0, 210.0, 0.0, 180.0, 0.0])
        self.filter = MeanFilter(12)
        self.filter.add(self.robot_init_pos)
        self.Flag = True
        self.robot_following = False
        # mp config
        self.mp_drawing = mp.solutions.drawing_utils
        self.mp_face_mesh = mp.solutions.face_mesh
        self.drawing_spec = self.mp_drawing.DrawingSpec(thickness=1, circle_radius=1)

        if USEHAND:
            self.mpHands = mp.solutions.hands
            self.hands = self.mpHands.Hands()  # 设置参数，详见 hands.py 中的 __init__
            self.mpDraw = mp.solutions.drawing_utils  # 将检测出的手上的标记点连接起来

    def robot_into_follow(self):
        if USEROBOT:
            self.robot_following = True

    def robot_exit_follow(self):
        if USEROBOT:
            self.robot_following = False

    def stop(self):
        self.Flag = False
        self.ca.stop_kinect()
        if USEROBOT:
            self.robot.stop()

    def face_mark(self, image, flip=-1):
        image_roi = image[cymin:cymax, cxmin:cxmax, :]
        with self.mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5) as face_mesh:
            # image.flags.writeable = False
            results = face_mesh.process(image_roi)
            # image.flags.writeable = True
            if results.multi_face_landmarks:
                for face_landmarks in results.multi_face_landmarks:
                    # for id, lm in enumerate(face_landmarks.landmark):
                    # cx, cy = int(lm.x * w), int(lm.y * h)
                    left_eye_pos = np.array([0.0, 0.0, 0.0])
                    right_eye_pos = np.array([0.0, 0.0, 0.0])
                    nose_pos = np.array([0.0, 0.0, 0.0])
                    mouth_pos = np.array([0.0, 0.0, 0.0])
                    h, w, c = image_roi.shape  # 720, 1280, 3
                    k = 0
                    for id in left_eye_ids:
                        lm = face_landmarks.landmark[id]
                        cx, cy = int(lm.x * w) + cxmin, int(lm.y * h) + cymin
                        cv2.circle(image_roi, [cx-cxmin, cy-cymin],5,(255,0,0))
                        p = self.ca.color2point([cx, cy], vflip=True, hflip=True)
                        if any(p):
                            k += 1
                            left_eye_pos += p
                    if k == 0:
                        continue
                    left_eye_pos /= k

                    k=0
                    for id in right_eye_ids:
                        lm = face_landmarks.landmark[id]
                        cx, cy = int(lm.x * w) + cxmin, int(lm.y * h) + cymin
                        cv2.circle(image_roi, [cx - cxmin, cy - cymin], 5, (255, 0, 0))
                        p = self.ca.color2point([cx, cy], vflip=True, hflip=True)
                        if any(p):
                            k += 1
                            right_eye_pos += p
                    if k == 0:
                        continue
                    right_eye_pos /= k

                    k = 0
                    for id in nose_ids:
                        lm = face_landmarks.landmark[id]
                        cx, cy = int(lm.x * w) + cxmin, int(lm.y * h) + cymin
                        cv2.circle(image_roi, [cx - cxmin, cy - cymin], 5, (255, 0, 0))
                        p = self.ca.color2point([cx, cy], vflip=True, hflip=True)
                        if any(p):
                            k += 1
                            nose_pos += p
                    if k == 0:
                        continue
                    nose_pos /= k

                    k = 0
                    for id in mouth_ids:
                        lm = face_landmarks.landmark[id]
                        cx, cy = int(lm.x * w) + cxmin, int(lm.y * h) + cymin
                        cv2.circle(image_roi, [cx - cxmin, cy - cymin], 5, (255, 0, 0))
                        p = self.ca.color2point([cx, cy], vflip=True, hflip=True)
                        if any(p):
                            k += 1
                            mouth_pos += p
                    if k == 0:
                        continue
                    mouth_pos /= k

                    # print("左右眼鼻口：", left_eye_pos, right_eye_pos, nose_pos, mouth_pos)

                    # 旋转平移矩阵
                    headX = right_eye_pos - left_eye_pos
                    headY = np.cross(left_eye_pos-mouth_pos, right_eye_pos-mouth_pos)
                    headX = headX / np.linalg.norm(headX)
                    headY = headY / np.linalg.norm(headY)
                    headZ = np.cross(headX, headY)
                    dist = nose_pos
                    # print("x y z:", headX, headY, headZ)
                    R_ca2head = np.vstack([headX, headY, headZ]).T
                    # print("R:", R_ca2head)

                    # p = np.dot(np.linalg.inv(R_ca2head), np.array([-130, 150, 120])) + dist
                    # head_normal = np.dot(np.linalg.inv(R_ca2head), np.array([-1, 0, 2]))
                    #
                    p = np.dot(np.linalg.inv(R_ca2head), np.array([-10, 150, 180])) + dist
                    head_normal = np.dot(np.linalg.inv(R_ca2head), np.array([0, 0, -1]))

                    head_normal /= np.linalg.norm(head_normal)

                    self.mp_drawing.draw_landmarks(
                        image=image_roi,
                        landmark_list=face_landmarks,
                        connections=self.mp_face_mesh.FACE_CONNECTIONS,
                        landmark_drawing_spec=self.drawing_spec,
                        connection_drawing_spec=self.drawing_spec)

                    image[cymin:cymax, cxmin:cxmax, :] = image_roi

                    return p, head_normal, image

            return None, None, image

    def get_handpos(self, img):
        """
        输入：rgb_img
        输出：手的位置--相机坐标系，法向量--相机坐标系，手的图像
        """
        # imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        image_roi = img[cymin:cymax, cxmin:cxmax, :]
        results = self.hands.process(image_roi)  # 对输入图像进行处理，探索图像中是否有手
        h, w, c = image_roi.shape
        if results.multi_hand_landmarks:
            for handLms in results.multi_hand_landmarks:  # 捕捉画面中的每一只手
                # 取0, 5, 13 三个关节计算手坐标系
                id = 0
                lm = handLms.landmark[id]
                # for id, lm in enumerate(handLms.landmark):
                cx, cy = int(lm.x * w) + cxmin, int(lm.y * h) + cymin  # 根据比例还原出每一个标记点的像素坐标
                p0 = self.ca.color2point([cx, cy], vflip=True, hflip=True)
                if p0 is None or any(np.isnan(p0)) or any(np.isinf(p0)):
                    continue
                p0[2] += 0.010  # 0 节点太翘

                id = 5
                lm = handLms.landmark[id]
                # for id, lm in enumerate(handLms.landmark):
                cx, cy = int(lm.x * w) + cxmin, int(lm.y * h) + cymin  # 根据比例还原出每一个标记点的像素坐标
                p5 = self.ca.color2point([cx, cy], vflip=True, hflip=True)
                if p5 is None or any(np.isnan(p5)) or any(np.isinf(p5)):
                    continue

                id = 13
                lm = handLms.landmark[id]
                # for id, lm in enumerate(handLms.landmark):
                cx, cy = int(lm.x * w) + cxmin, int(lm.y * h) + cymin  # 根据比例还原出每一个标记点的像素坐标
                p13 = self.ca.color2point([cx, cy], vflip=True, hflip=True)
                if p13 is None or any(np.isnan(p13)) or any(np.isinf(p13)):
                    continue

                hand_pos = (p0+p5+p13)/3
                hand_nomal = 1e3*np.cross(p5-p0, p13-p0)
                if hand_nomal is None or not any(hand_nomal):
                    continue
                hand_nomal /= np.linalg.norm(hand_nomal)
                # print("hp", hand_pos)
                # print("hn", hand_nomal)
                self.mpDraw.draw_landmarks(image_roi, handLms, self.mpHands.HAND_CONNECTIONS)  # 给画面中的每一只手进行标点、连线的操作
                img[cymin:cymax, cxmin:cxmax, :] = image_roi
                return hand_pos, hand_nomal, img
        return None, None, img

    def run(self):
        time.sleep(1)
        last_pos = self.robot_init_pos
        follow = False
        # last_pos = self.robot_init_pos[:3]
        while self.Flag:
            if USEROBOT and self.robot_following and not follow:
                self.robot.intofollowTVG(speed=SPEED)
                follow = True
                self.robot_can_move = True
            if USEROBOT and not self.robot_following and follow:
                self.robot.exitfollowTVG()
                follow = False
                self.robot_can_move = False
            if USEROBOT and self.robot_can_move:

                x_tar = self.filter.get()
                x_sta = last_pos
                dist = np.linalg.norm(x_tar[0:3] - x_sta[0:3])
                ds = dist
                p = (x_tar - x_sta) * ds / dist
                x_v = p + x_sta
                pos = x_v
                print(
                    "G40" + " X=" + str(round(pos[0], 1)) + " Y=" + str(round(pos[1], 1)) + " Z=" + str(
                        round(pos[2], 1)) + " A=" + str(round(pos[3], 1)) + " B=" + str(
                        round(pos[4], 1)) + " C=" + str(
                        round(pos[5], 1)))
                self.robot.movepGlineTVG(x_v, Gline="G40", wait=True)
                last_pos = x_v + 0.0

            time.sleep(0.05)
        self.robot.exitfollowTVG()
        print("model stop")


class Controller(threading.Thread):

    def __init__(self):
        threading.Thread.__init__(self)
        self.model = Model()
        self.view = MainWindow()
        # 绑定按钮事件
        self.view.ui.pb1.clicked.connect(self.start_robot)
        # self.view.ui.pb2.clicked.connect(self.changeMode)
        # self.view.ui.pb3.clicked.connect(self.openFile)
        self.view.ui.pb4.clicked.connect(self.stop)

        self.view.ui.pb_zt.clicked.connect(lambda: self.robot_sendcmd("zt"))
        self.view.ui.pb_tc.clicked.connect(lambda: self.robot_sendcmd("tc"))
        self.view.ui.pb_sj.clicked.connect(lambda: self.robot_sendcmd("sj"))
        self.view.ui.pb_fw.clicked.connect(lambda: self.robot_sendcmd("fw"))
        self.view.ui.pb_cq.clicked.connect(lambda: self.robot_sendcmd("cq"))
        self.view.ui.pb_yx.clicked.connect(lambda: self.robot_sendcmd("yx"))

        self.view.ui.pb_setspeed.clicked.connect(self.robot_setspeed)
        self.view.ui.pb_j1a.pressed.connect(lambda: self.robot_jog("J1", "+"))
        self.view.ui.pb_j2a.pressed.connect(lambda: self.robot_jog("J2", "+"))
        self.view.ui.pb_j3a.pressed.connect(lambda: self.robot_jog("J3", "+"))
        self.view.ui.pb_j4a.pressed.connect(lambda: self.robot_jog("J4", "+"))
        self.view.ui.pb_j5a.pressed.connect(lambda: self.robot_jog("J5", "+"))
        self.view.ui.pb_j6a.pressed.connect(lambda: self.robot_jog("J6", "+"))
        self.view.ui.pb_j1d.pressed.connect(lambda: self.robot_jog("J1", "-"))
        self.view.ui.pb_j2d.pressed.connect(lambda: self.robot_jog("J2", "-"))
        self.view.ui.pb_j3d.pressed.connect(lambda: self.robot_jog("J3", "-"))
        self.view.ui.pb_j4d.pressed.connect(lambda: self.robot_jog("J4", "-"))
        self.view.ui.pb_j5d.pressed.connect(lambda: self.robot_jog("J5", "-"))
        self.view.ui.pb_j6d.pressed.connect(lambda: self.robot_jog("J6", "-"))
        self.view.ui.pb_txa.pressed.connect(lambda: self.robot_jog("TX", "+"))
        self.view.ui.pb_txd.pressed.connect(lambda: self.robot_jog("TX", "-"))
        self.view.ui.pb_tya.pressed.connect(lambda: self.robot_jog("TY", "+"))
        self.view.ui.pb_tyd.pressed.connect(lambda: self.robot_jog("TY", "-"))
        self.view.ui.pb_tza.pressed.connect(lambda: self.robot_jog("TZ", "+"))
        self.view.ui.pb_tzd.pressed.connect(lambda: self.robot_jog("TZ", "-"))
        self.view.ui.pb_taa.pressed.connect(lambda: self.robot_jog("TA", "+"))
        self.view.ui.pb_tad.pressed.connect(lambda: self.robot_jog("TA", "-"))
        self.view.ui.pb_tba.pressed.connect(lambda: self.robot_jog("TB", "+"))
        self.view.ui.pb_tbd.pressed.connect(lambda: self.robot_jog("TB", "-"))
        self.view.ui.pb_tca.pressed.connect(lambda: self.robot_jog("TC", "+"))
        self.view.ui.pb_tcd.pressed.connect(lambda: self.robot_jog("TC", "-"))

        self.view.ui.pb_j1a.released.connect(lambda: self.robot_sendcmd("zt"))
        self.view.ui.pb_j2a.released.connect(lambda: self.robot_sendcmd("zt"))
        self.view.ui.pb_j3a.released.connect(lambda: self.robot_sendcmd("zt"))
        self.view.ui.pb_j4a.released.connect(lambda: self.robot_sendcmd("zt"))
        self.view.ui.pb_j5a.released.connect(lambda: self.robot_sendcmd("zt"))
        self.view.ui.pb_j6a.released.connect(lambda: self.robot_sendcmd("zt"))
        self.view.ui.pb_j1d.released.connect(lambda: self.robot_sendcmd("zt"))
        self.view.ui.pb_j2d.released.connect(lambda: self.robot_sendcmd("zt"))
        self.view.ui.pb_j3d.released.connect(lambda: self.robot_sendcmd("zt"))
        self.view.ui.pb_j4d.released.connect(lambda: self.robot_sendcmd("zt"))
        self.view.ui.pb_j5d.released.connect(lambda: self.robot_sendcmd("zt"))
        self.view.ui.pb_j6d.released.connect(lambda: self.robot_sendcmd("zt"))
        self.view.ui.pb_txa.released.connect(lambda: self.robot_sendcmd("zt"))
        self.view.ui.pb_txd.released.connect(lambda: self.robot_sendcmd("zt"))
        self.view.ui.pb_tya.released.connect(lambda: self.robot_sendcmd("zt"))
        self.view.ui.pb_tyd.released.connect(lambda: self.robot_sendcmd("zt"))
        self.view.ui.pb_tza.released.connect(lambda: self.robot_sendcmd("zt"))
        self.view.ui.pb_tzd.released.connect(lambda: self.robot_sendcmd("zt"))
        self.view.ui.pb_taa.released.connect(lambda: self.robot_sendcmd("zt"))
        self.view.ui.pb_tad.released.connect(lambda: self.robot_sendcmd("zt"))
        self.view.ui.pb_tba.released.connect(lambda: self.robot_sendcmd("zt"))
        self.view.ui.pb_tbd.released.connect(lambda: self.robot_sendcmd("zt"))
        self.view.ui.pb_tca.released.connect(lambda: self.robot_sendcmd("zt"))
        self.view.ui.pb_tcd.released.connect(lambda: self.robot_sendcmd("zt"))
        self.Flag = True
        self.view.show()
        self.R = np.array([[0.0634808, -0.63111266, -0.77308926],
             [-0.99758594, -0.06197996, -0.0313175],
             [-0.02815117,  0.77321103, -0.63352365]])
        self.T = np.array([[1109.77197985],
             [76.53493653],
             [616.18083264]])
        if USEROBOT:
            self.model.start()
        time.sleep(1)

    def robot_jog(self, joint, dir):
        if USEROBOT:
            self.model.robot.jogTVG(joint, dir)
            # time.sleep(0.05)
            # self.model.robot.pauseTVG()

    def robot_sendcmd(self, cmd):
        if USEROBOT:
            self.model.robot.send_cmd(cmd)

    def robot_setspeed(self):
        if USEROBOT:
            speed = int(self.view.ui.lineEdit.text())
            if speed <= 0:
                speed = 1
            if speed > 100:
                speed = 100
            self.model.robot.setSpeedTVG(speed)

    def start_robot(self):
        if not self.model.robot_can_move:
            if USEROBOT:
                self.model.robot_into_follow()
            self.view.ui.pb1.setText("结束跟踪")
        else:
            if USEROBOT:
                self.model.robot_exit_follow()
            self.view.ui.pb1.setText("启动跟踪")

    def stop(self):
        self.Flag = False
        self.view.close()
        self.model.stop()

    def run(self):

        time.sleep(1.5)
        last_pos = self.model.robot_init_pos

        while self.Flag:
            time.sleep(0.03)
            if self.view.isclosed:
                self.model.stop()
                break

            color_img = self.model.ca.get_img("c")
            if color_img is None or not np.any(color_img):
                continue

            color_img = cv2.cvtColor(color_img[:, :, :3], cv2.COLOR_BGR2RGB)
            color_img = cv2.flip(color_img, -1)
            # flip > 0 水平翻转 ,flip =0 垂直翻转 ,flip< 0水平和垂直翻转

            if USEHAND:
                hand_pos, hand_normal, color_img = self.model.get_handpos(color_img)
                # print("hand_pos:", hand_pos)
                self.view.showCameraPos(hand_pos)
                if hand_pos is not None and self.model.robot_can_move:
                    robot_pos = np.dot(self.R, hand_pos) + self.T.reshape(1, 3)
                    robot_pos = robot_pos[0]
                    # print("robot_pos:", robot_pos)
                    self.view.showRobotPos(robot_pos)
                    robot_normal = np.dot(self.R, hand_normal)
                    # # print(robot_normal)
                    # if robot_normal[2] > 0:
                    #     # 朝下
                    #     robot_normal = -robot_normal
                    #
                    # # theta = math.pi * 30 / 180  # 若 normal 倾斜超过 theta, 矫正到 theta 内
                    # # robot_z_vector = np.array([0, 0, -1])
                    # # if robot_z_vector.dot(robot_normal) < math.cos(theta):
                    # #
                    # #     if robot_normal[0] < 1e-4:
                    # #         sign = 1 if robot_normal[1] > 0 else -1
                    # #         a = np.array([0, sign * math.sin(theta), -math.cos(theta)])
                    # #         robot_normal = a
                    # #     else:
                    # #         sign = 1 if robot_normal[0] > 0 else -1
                    # #         t = robot_normal[1] / robot_normal[0]
                    # #         a = np.array([sign * 1, sign * t, -math.sqrt((1 + t ** 2)) / math.tan(theta)])
                    # #         robot_normal = a / np.linalg.norm(a)
                    #
                    robot_pos -= 100 * robot_normal
                    # robot_pos[2] += 120
                    # print(robot_normal)
                    if rxmin <= robot_pos[0] <= rxmax and rymin <= robot_pos[1] <= rymax and rzmin <= robot_pos[2] <= rzmax:

                        A, B, C = self.model.robot.eeNormal2ABC(robot_normal)
                        pos = np.array([robot_pos[0], robot_pos[1], robot_pos[2], A, B, C])
                        # pos = robot_pos
                        self.model.filter.add(pos)


            if USEFACE:
                head_pos, head_normal, color_img = self.model.face_mark(color_img)
                self.view.showCameraPos(head_pos)
                if head_pos is not None :#and self.model.robot_can_move:
                    robot_pos = np.dot(self.R, head_pos) + self.T.reshape(1, 3)
                    robot_pos = robot_pos[0]
                    # print("robot_pos", robot_pos)
                    self.view.showRobotPos(robot_pos)

                    robot_normal = np.dot(self.R, head_normal)
                    # print(robot_normal)
                    if robot_normal[2] > 0:
                        # 朝下
                        robot_normal = -robot_normal

                    theta = math.pi * 45 / 180  # 若 normal 倾斜超过 theta, 矫正到 theta 内
                    robot_z_vector = np.array([0, 0, -1])
                    if robot_z_vector.dot(robot_normal) < math.cos(theta):
                        if robot_normal[0] < 1e-4:
                            sign = 1 if robot_normal[1] > 0 else -1
                            a = np.array([0, sign * math.sin(theta), -math.cos(theta)])
                            robot_normal = a
                        else:
                            sign = 1 if robot_normal[0] > 0 else -1
                            t = robot_normal[1] / robot_normal[0]
                            a = np.array([sign * 1, sign * t, -math.sqrt((1 + t ** 2)) / math.tan(theta)])
                            robot_normal = a / np.linalg.norm(a)

                    robot_pos -= 80 * robot_normal
                    if rxmin <= robot_pos[0] <= rxmax and rymin <= robot_pos[1] <= rymax and rzmin <= robot_pos[2] <= rzmax:
                        print(robot_normal)
                        A, B, C = self.model.robot.eeNormal2ABC(robot_normal)
                        pos = np.array([robot_pos[0], robot_pos[1], robot_pos[2], A, B, C])
                        # print("G40"+" X="+str(pos[0])+" Y="+str(pos[1])+" Z="+str(pos[2])+" A="+str(pos[3])+" B="+str(pos[4])+" C="+str(pos[5]))
                        self.model.filter.add(pos)

            height, width, channel = color_img.shape
            color_img = cv2.resize(color_img, (width//2, height//2))
            img = QImage(color_img.data, width//2, height//2, width//2*channel, QImage.Format_RGB888)
            self.view.showrgbView(img)

            # x_tar = self.model.filter.get()
            #
            # x_sta = last_pos
            # dist = np.linalg.norm(x_tar[0:3] - x_sta[0:3])
            #
            # if dist >= 8:
            #     if self.model.robot.moving:
            #         ds = 0.5
            #     else:
            #         ds = 8
            #     p = (x_tar - x_sta) * ds / dist
            #     x_v = p + x_sta
            #     # self.model.robot.movepGlineTVG(x_v, "G10", wait=True)
            #     pos = x_v

                # print("G40" + " X=" + str(round(pos[0],1)) + " Y=" + str(round(pos[1],1)) + " Z=" + str(round(pos[2],1)) +
                #       " A=" + str(round(pos[3],1)) + " B=" + str(round(pos[4],1)) + " C=" + str(round(pos[5],1)))
                # last_pos = x_v + 0.0

            # if USEROBOT:
            #     if self.model.robot_can_move:
            #         x_tar = self.model.filter.get()
            #
            #         x_sta = last_pos
            #         dist = np.linalg.norm(x_tar[0:3] - x_sta[0:3])
            #
            #         if dist >= 8:
            #             if not self.model.robot.moving:
            #             #     ds = 0.5
            #             #     # p = (x_tar - x_sta) * ds / dist
            #             #     # x_v = p + x_sta
            #             #     # self.model.robot.movepGlineTVG(x_v[0:3], "G10", wait=True)
            #             #     # last_pos = x_v + 0.0
            #             #
            #             # else:
            #                 ds = 8
            #                 p = (x_tar - x_sta) * ds / dist
            #                 x_v = p + x_sta
            #                 pos = x_v
            #                 print("G40" + " X=" + str(round(pos[0], 1)) + " Y=" + str(round(pos[1], 1)) + " Z=" + str(
            #                     round(pos[2], 1)) + " A=" + str(round(pos[3], 1)) + " B=" + str(round(pos[4], 1)) + " C=" + str(
            #                     round(pos[5], 1)))
            #                 self.model.robot.movepGlineTVG(x_v, "G40", wait=True)
            #                 last_pos = x_v + 0.0

            if self.model.robot_can_move:
                self.view.showDebugInfo("robot运行")
            else:
                self.view.showDebugInfo("robot停止")


class MainWindow(QMainWindow):
    def __init__(self):

        super(MainWindow, self).__init__()
        self.ui = Ui_MainWindow()
        self.ui.setupUi(self)
        self.isclosed = False

        # ----> SET WINDOW TITLE AND ICON
        applicationName = "hello world"
        self.setWindowTitle(applicationName)

        self.vtkWidget = QVTKRenderWindowInteractor(self.ui.lbmodel)  # 创建3D模型加载窗口
        self.vtkWidget.setWindowOpacity(0)  # 设置透明度
        self.vtkWidget.resize(550, 540)  # 设置3D模型大小

        self.ui.lbvideo.setMaximumSize(550, 540)
        self.ui.lbvideo.setPixmap(QPixmap(550, 540))
        self.ui.lbvideo.setScaledContents(True)

        # filename = 'C:/Users/10401/Desktop/00.3DS'
        filename = "humanbrain.3DS"
        importer = vtk.vtk3DSImporter()
        # importer.ComputeNormalsOn()
        importer.SetFileName(filename)
        importer.Read()

        self.ren = importer.GetRenderer()
        self.ren.SetBackground(0.1, 0.2, 0.4)
        self.renwin = self.vtkWidget.GetRenderWindow()
        importer.SetRenderWindow(self.renwin)
        # self.renwin.Render()
        self.iren = self.renwin.GetInteractor()

        self.iren.Initialize()
        self.iren.Start()

        #############################################################

        self.ms = MySignal()
        self.ms.str_msg.connect(self._updateLabel)
        # self.ms.ndarray_msg.connect(self.updateCoordinate)
        # self.ms.int_msg.connect(self.updateBoxValue)
        self.ms.progressbar_int_msg.connect(self._updateProgressBar)
        self.ms.rgbviewer_msg.connect(self._updateView)

    def closeEvent(self, a0: QtGui.QCloseEvent) -> None:
        self.isclosed = True

    def _updateLabel(self, lb, info):
        lb.setText(info)

    def _updateProgressBar(self, pb, val):
        pb.setValue(val)

    def _updateView(self, lb, data):
        pix_img = QPixmap.fromImage(data)
        # pix_img = pix_img.scaled(550, 540, QtCore.Qt.KeepAspectRatio)
        lb.setPixmap(pix_img)
        lb.setScaledContents(True)

    def showrgbView(self, data):
        self.ms.rgbviewer_msg.emit(self.ui.lbvideo, data)

    def showDebugInfo(self, info):
        self.ms.str_msg.emit(self.ui.lb12, str(info))

    def showCameraPos(self, camera_pos):
        if camera_pos is not None:
            msg = str(round(camera_pos[0], 0)) + ", " \
                  + str(round(camera_pos[1], 0)) + ", "\
                  + str(round(camera_pos[2], 0))
        else:
            msg = " - , - , - "
        self.ms.str_msg.emit(self.ui.lb22, msg)

    def showRobotPos(self, robot_pos):
        if robot_pos is not None:
            msg = str(round(robot_pos[0], 0)) + ", "\
                  + str(round(robot_pos[1], 0)) + ", "\
                  + str(round(robot_pos[2], 0))
        else:
            msg = " - , - , - "
        self.ms.str_msg.emit(self.ui.lb32, msg)


if __name__ == "__main__":
    app = QApplication(sys.argv)
    logi = Controller()
    logi.start()
    sys.exit(app.exec_())
