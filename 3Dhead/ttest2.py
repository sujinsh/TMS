# 加载obj模型
# !/usr/bin/env python
import cv2
# import pyk4a
# from helpers import colorize
# from pyk4a import Config, PyK4A
import vtk
import mediapipe as mp
from vtk.qt.QVTKRenderWindowInteractor import QVTKRenderWindowInteractor
import sys
# IMPORTING ALL THE NECESSERY PYSIDE2 MODULES FOR OUR APPLICATION.
from PyQt5 import QtCore, QtGui, QtWidgets
from PyQt5.QtCore import *
from PyQt5.QtGui import *
from PyQt5.QtWidgets import *

from ui_tmsvison import Ui_MainWindow  # MAINWINDOW CODE GENERATED BY THE QT DESIGNER AND pyside2-uic.
from ttestkinect import Camera
from ttestur3 import UR3
# from ttestTVGrobot import TVGserial  # 机器人串口控制
from Filters import MeanFilter, WeightedFilter  # 滤波器
import math
from scipy.spatial.transform import Rotation as SSTR
# import matplotlib

# matplotlib.use("Qt5Agg")  # 声明使用QT5
# from matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas
# from matplotlib.figure import Figure
# from matplotlib import pyplot
import numpy as np
# from scipy.signal import butter, lfilter
import threading
import time
# import pickle
# import pandas as pd
# from wfdb import processing
# from tensorflow import keras
# from tensorflow.keras.layers import Dense, Input, Dropout, Convolution1D, MaxPool1D, GlobalMaxPool1D, \
#     GlobalAveragePooling1D, concatenate

USEROBOT = True
USEHAND = True
USEFACE = False

rxmin, rxmax = -0.180, 0.230
rymin, rymax = -0.480, -0.200
rzmin, rzmax = 0.0, 0.350

# 水平和垂直翻转后的roi图像区域
cxmin, cxmax = 380, 840
cymin, cymax = 100, 700

face_ids = [234, 127, 162, 21, 54, 103, 67, 109, 10, 338, 297, 332, 284, 251, 389, 93, 132, 58, 138, 172, 136, 150, 149, 176, 148, 152, 377, 400, 378, 379, 365, 397, 288, 361, 323, 454, 356]
# 面向患者视角的左右眼， 图像的左右
# left_eye_ids = [229, 230, 231, 224, 223, 222]
# right_eye_ids = [444, 443, 442, 449, 450, 451]
# nose_ids = [4,5,45,275]
# mouth_ids = [0,18, 182, 406]
#
left_eye_ids = [229, 230, 231, 224, 223, 222]
right_eye_ids = [444, 443, 442, 449, 450, 451]
nose_ids = [4,5,45,275]
mouth_ids = [151]

class MySignal(QObject):
    str_msg = pyqtSignal(QLabel, str)
    ndarray_msg = pyqtSignal(np.ndarray)
    progressbar_int_msg = pyqtSignal(QProgressBar, int)
    int_msg = pyqtSignal(int)
    rgbviewer_msg = pyqtSignal(QLabel, QImage)


class Model(threading.Thread):
    def __init__(self):
        threading.Thread.__init__(self)
        self.ca = Camera()
        self.ca.start()
        # USE UR3Robot
        if USEROBOT:
            self.robot = UR3("192.168.1.3", track_threshold=0.05, payload=0.9)
            self.robot.start()
        time.sleep(0.1)
        print("启动机器人")
        self.robot_can_move = False
        # self.robot_init_pos = np.array([283.0, 0.0, 210.0, 0.0, 180.0, 0.0])
        self.filter = MeanFilter(30)
        self.filter_nose = MeanFilter(30)
        self.filter_test = MeanFilter(10)
        # self.filter.add(self.robot_init_pos)
        self.Flag = True

        # mp config
        self.mp_drawing = mp.solutions.drawing_utils
        self.mp_face_mesh = mp.solutions.face_mesh
        self.drawing_spec = self.mp_drawing.DrawingSpec(thickness=1, circle_radius=1)

        if USEHAND:
            self.mpHands = mp.solutions.hands
            self.hands = self.mpHands.Hands()  # 设置参数，详见 hands.py 中的 __init__
            self.mpDraw = mp.solutions.drawing_utils  # 将检测出的手上的标记点连接起来

    def robot_into_follow(self):
        if USEROBOT:
            self.robot_can_move = True
            self.robot.canrun = True

    def robot_exit_follow(self):
        if USEROBOT:
            self.robot_can_move = False
            self.robot.canrun = False

    def stop(self):
        self.Flag = False
        self.ca.stop_kinect()
        if USEROBOT:
            self.robot.stop()

    def face_mark(self, image):
        scale = 10
        image_roi = image[cymin:cymax, cxmin:cxmax, :]
        h, w, c = image_roi.shape
        image_roi = cv2.resize(image_roi, (w*scale, h*scale))
        with self.mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5) as face_mesh:
            # image.flags.writeable = False
            results = face_mesh.process(image_roi)
            # image.flags.writeable = True
            if results.multi_face_landmarks:
                for face_landmarks in results.multi_face_landmarks:
                    # for id, lm in enumerate(face_landmarks.landmark):
                    # cx, cy = int(lm.x * w), int(lm.y * h)
                    left_eye_pos = np.array([0.0, 0.0, 0.0])
                    right_eye_pos = np.array([0.0, 0.0, 0.0])
                    nose_pos = np.array([0.0, 0.0, 0.0])
                    mouth_pos = np.array([0.0, 0.0, 0.0])
                    h, w, c = image_roi.shape  # 720, 1280, 3
                    h /= scale
                    w /= scale
                    k = 0
                    for id in left_eye_ids:
                        lm = face_landmarks.landmark[id]
                        cx, cy = int(lm.x * w) + cxmin, int(lm.y * h) + cymin
                        # cv2.circle(image_roi, [cx-cxmin, cy-cymin],5,(255,0,0))
                        p = self.ca.color2point([cx, cy], vflip=True, hflip=True)
                        if np.all(p < 1000):
                            k += 1
                            left_eye_pos += p
                    if k == 0:
                        continue
                    left_eye_pos /= k

                    k=0
                    for id in right_eye_ids:
                        lm = face_landmarks.landmark[id]
                        cx, cy = int(lm.x * w) + cxmin, int(lm.y * h) + cymin
                        # cv2.circle(image_roi, [cx - cxmin, cy - cymin], 5, (255, 0, 0))
                        p = self.ca.color2point([cx, cy], vflip=True, hflip=True)
                        if np.all(p < 1000):
                            k += 1
                            right_eye_pos += p
                    if k == 0:
                        continue
                    right_eye_pos /= k

                    k = 0
                    for id in nose_ids:
                        lm = face_landmarks.landmark[id]
                        cx, cy = int(lm.x * w) + cxmin, int(lm.y * h) + cymin
                        # cv2.circle(image_roi, [cx - cxmin, cy - cymin], 5, (255, 0, 0))
                        p = self.ca.color2point([cx, cy], vflip=True, hflip=True)
                        if np.all(p < 1000):
                            k += 1
                            nose_pos += p
                    if k == 0:
                        continue
                    nose_pos /= k
                    self.filter_nose.add(nose_pos)
                    nose_pos = self.filter_nose.get()

                    k = 0
                    for id in mouth_ids:
                        lm = face_landmarks.landmark[id]
                        cx, cy = int(lm.x * w) + cxmin, int(lm.y * h) + cymin
                        # cv2.circle(image_roi, [cx - cxmin, cy - cymin], 5, (255, 0, 0))
                        p = self.ca.color2point([cx, cy], vflip=True, hflip=True)
                        if np.all(p < 1000):
                            k += 1
                            mouth_pos += p
                    if k == 0:
                        continue
                    mouth_pos /= k

                    id = 8  # 测试id
                    lm = face_landmarks.landmark[id]
                    cx, cy = int(lm.x * w) + cxmin, int(lm.y * h) + cymin
                    # cv2.circle(image_roi, [cx - cxmin, cy - cymin], 10, (0, 0, 255))
                    p = self.ca.color2point([cx, cy], vflip=True, hflip=True)
                    # print("左右眼鼻口：", left_eye_pos, right_eye_pos, nose_pos, mouth_pos)

                    # 旋转平移矩阵
                    headX = right_eye_pos - left_eye_pos
                    self.filter.add(headX)
                    headX = self.filter.get()

                    headY = -np.cross(left_eye_pos-mouth_pos, right_eye_pos-mouth_pos)
                    headX = headX / np.linalg.norm(headX)
                    headY = headY / np.linalg.norm(headY)
                    headZ = np.cross(headX, headY)
                    t = nose_pos
                    # print("x y z:", headX, headY, headZ)
                    R_ca2head = np.vstack([headX, headY, headZ]).T
                    # print("R:", R_ca2head)
                    # R_head2ca = np.linalg.inv(R_ca2head)
                    # print(np.dot(R_ca2head, (p-t).reshape(3,1)))

                    # cam_p = np.vstack([headX, headY, headZ])
                    # R_ca2head_inv = cam_p - np.vstack([t, t, t])
                    # R_ca2head = np.linalg.inv(R_ca2head_inv)
                    #
                    # T_ca2head = -np.matmul(t, R_ca2head)
                    p = np.matmul(p - t, R_ca2head)
                    print('p:', p)

                    image_roi = cv2.resize(image_roi, (int(w), int(h)))
                    self.mp_drawing.draw_landmarks(
                        image=image_roi,
                        landmark_list=face_landmarks,
                        connections=self.mp_face_mesh.FACE_CONNECTIONS,
                        landmark_drawing_spec=self.drawing_spec,
                        connection_drawing_spec=self.drawing_spec)

                    image[cymin:cymax, cxmin:cxmax, :] = image_roi
                    if abs(p[0]) > 10:
                        print("转动过大")
                        return None, None, image
                    return R_ca2head, t, image

            return None, None, image

    def get_handpos(self, img):
        """
        输入：rgb_img
        输出：手的位置--相机坐标系，法向量--相机坐标系，手的图像
        """
        # imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        image_roi = img[cymin:cymax, cxmin:cxmax, :]
        results = self.hands.process(image_roi)  # 对输入图像进行处理，探索图像中是否有手
        h, w, c = image_roi.shape
        if results.multi_hand_landmarks:
            for handLms in results.multi_hand_landmarks:  # 捕捉画面中的每一只手
                # 取0, 5, 13 三个关节计算手坐标系
                id = 0
                lm = handLms.landmark[id]
                # for id, lm in enumerate(handLms.landmark):
                cx, cy = int(lm.x * w) + cxmin, int(lm.y * h) + cymin  # 根据比例还原出每一个标记点的像素坐标
                p0 = self.ca.color2point([cx, cy], vflip=True, hflip=True)
                if p0 is None or any(np.isnan(p0)) or any(np.isinf(p0)):
                    continue
                # p0[2] += 0.010  # 0 节点太翘

                id = 5
                lm = handLms.landmark[id]
                # for id, lm in enumerate(handLms.landmark):
                cx, cy = int(lm.x * w) + cxmin, int(lm.y * h) + cymin  # 根据比例还原出每一个标记点的像素坐标
                p5 = self.ca.color2point([cx, cy], vflip=True, hflip=True)
                if p5 is None or any(np.isnan(p5)) or any(np.isinf(p5)):
                    continue

                id = 13
                lm = handLms.landmark[id]
                # for id, lm in enumerate(handLms.landmark):
                cx, cy = int(lm.x * w) + cxmin, int(lm.y * h) + cymin  # 根据比例还原出每一个标记点的像素坐标
                p13 = self.ca.color2point([cx, cy], vflip=True, hflip=True)
                if p13 is None or any(np.isnan(p13)) or any(np.isinf(p13)):
                    continue

                hand_pos = (p0+p5+p13)/3
                hand_nomal = 1e3*np.cross(p5-p0, p13-p0)
                if hand_nomal is None or not any(hand_nomal):
                    continue
                hand_nomal /= np.linalg.norm(hand_nomal)
                # print("hp", hand_pos)
                # print("hn", hand_nomal)
                self.mpDraw.draw_landmarks(image_roi, handLms, self.mpHands.HAND_CONNECTIONS)  # 给画面中的每一只手进行标点、连线的操作
                img[cymin:cymax, cxmin:cxmax, :] = image_roi
                return hand_pos, hand_nomal, img
        return None, None, img

    def run(self):
        time.sleep(1)

        while self.Flag:
            pass
            time.sleep(0.1)
        print("model stop")


class Controller(threading.Thread):

    def __init__(self):
        threading.Thread.__init__(self)
        self.model = Model()
        self.view = MainWindow()
        # 绑定按钮事件
        self.view.ui.pb1.clicked.connect(self.start_robot)
        # self.view.ui.pb2.clicked.connect(self.changeMode)
        # self.view.ui.pb3.clicked.connect(self.openFile)
        self.view.ui.pb4.clicked.connect(self.stop)
        self.view.ui.pushButton_6.clicked.connect(self.readdata)

        self.head_pos = None
        self.head_normal = None

        self.Flag = True
        self.view.show()
        self.R = np.array([[-0.99926241, -0.03301532,  0.01961168],
             [-0.00365721,  0.59020813,  0.80724283],
             [-0.03822635 , 0.8065757,  -0.58989354]])
        self.T = np.array([[14.97876835],
             [-1174.31569749],
             [594.91603348]])
        if USEROBOT:
            self.model.start()
        time.sleep(1)

    def start_robot(self):
        if not self.model.robot_can_move:
            if USEROBOT:
                self.model.robot_into_follow()
            self.view.ui.pb1.setText("结束跟踪")
        else:
            if USEROBOT:
                self.model.robot_exit_follow()
            self.view.ui.pb1.setText("启动跟踪")

    def stop(self):
        self.Flag = False
        self.view.close()
        self.model.stop()

    def readdata(self):
        posx = self.view.ui.pos_x.text()
        posy = self.view.ui.pos_y.text()
        posz = self.view.ui.pos_z.text()
        norx = self.view.ui.nor_x.text()
        nory = self.view.ui.nor_y.text()
        norz = self.view.ui.nor_z.text()
        # if "" not in [posx, posy, posz, norx, nory, norz]:
        try:
            self.head_pos = np.array([float(posx), float(posy), float(posz)])
            self.head_normal = np.array([float(norx), float(nory), float(norz)])
            print("跟踪：",self.head_pos, self.head_normal)
        except:
            print("输入错误")
            self.head_pos = None
            self.head_normal = None

    def run(self):

        time.sleep(1.5)

        while self.Flag:
            time.sleep(0.03)
            if self.view.isclosed:
                self.model.stop()
                break

            color_img = self.model.ca.get_img("c")
            if color_img is None or not np.any(color_img):
                continue

            color_img = cv2.cvtColor(color_img[:, :, :3], cv2.COLOR_BGR2RGB)
            color_img = cv2.flip(color_img, -1)
            # flip > 0 水平翻转 ,flip =0 垂直翻转 ,flip< 0水平和垂直翻转

            if USEHAND:
                hand_pos, hand_normal, color_img = self.model.get_handpos(color_img)
                # print("hand_pos:", hand_pos)
                self.view.showCameraPos(hand_pos)
                if hand_pos is not None:# and self.model.robot_can_move:
                    robot_pos = np.dot(self.R, hand_pos) + self.T.reshape(1, 3)
                    # print("robot_pos:", robot_pos)
                    robot_pos = robot_pos[0]/1000

                    self.view.showRobotPos(robot_pos)
                    robot_normal = np.dot(self.R, hand_normal)
                    # print(robot_normal)
                    if robot_normal[2] > 0:
                        # 朝下
                        robot_normal = -robot_normal

                    theta = math.pi * 30 / 180  # 若 normal 倾斜超过 theta, 矫正到 theta 内
                    robot_z_vector = np.array([0, 0, -1])
                    if robot_z_vector.dot(robot_normal) < math.cos(theta):
                        nx, ny = robot_normal[0], robot_normal[1]
                        al = math.sin(theta)
                        l = math.sqrt(nx*nx + ny*ny)
                        x, y = al/l*nx, al/l*ny
                        a = np.array([x, y, -math.cos(theta)])
                        robot_normal = a/np.linalg.norm(a)

                    # 末端旋转矩阵
                    # TZ = robot_normal
                    # if TZ[0] < 1e-4 and TZ[1] < 1e-4:
                    #     TZ = np.array([0, 0, -1])
                    #     TX = np.array([1, 0, 0])
                    # else:
                    #     t = math.sqrt(TZ[0] ** 2 + TZ[2] ** 2)
                    #     TX = np.array([TZ[2] / t, 0, -TZ[0] / t])
                    # TY = np.cross(TZ, TX)
                    TY = robot_normal
                    if TY[0] < 1e-4 and TY[1] < 1e-4:
                        TY = np.array([0, 0, -1])
                        TX = np.array([-1, 0, 0])
                    else:
                        t = math.sqrt(TY[0] ** 2 + TY[2] ** 2)
                        TX = np.array([TY[2] / t, 0, -TY[0] / t])
                    TZ = np.cross(TX, TY)

                    M = np.array([TX, TY, TZ]).T

                    R = SSTR.from_matrix(M)
                    rotvet = R.as_rotvec()

                    Rx, Ry, Rz = rotvet[0], rotvet[1], rotvet[2]
                    # robot_pos[2] += 120
                    robot_pos -= 0.2 * robot_normal

                    # print("robot_normal", robot_normal)
                    if USEROBOT and rxmin <= robot_pos[0] <= rxmax and rymin <= robot_pos[1] <= rymax and rzmin <= robot_pos[2] <= rzmax:
                        pos = np.array([robot_pos[0], robot_pos[1], robot_pos[2], Rx, Ry, Rz])
                        self.model.robot.filter.add(pos)

            if USEFACE:
                # R_head2ca, T_head2ca, color_img = self.model.face_mark(color_img)
                R_ca2head, T_ca2head, color_img = self.model.face_mark(color_img)
                if R_ca2head is not None and self.head_pos is not None and self.head_normal is not None:
                    R_ca2head_inv = np.linalg.inv(R_ca2head)
                    cam_head_pos = np.matmul(self.head_pos, R_ca2head_inv) + T_ca2head

                    cam_head_normal = np.matmul(self.head_normal, R_ca2head_inv)
                    cam_head_normal /= np.linalg.norm(cam_head_normal)
                    self.view.showCameraPos(cam_head_pos)

                    robot_pos = np.matmul(self.R, cam_head_pos) + self.T.reshape(1, 3)
                    robot_pos = robot_pos[0] /1000

                    # print("robot_pos", robot_pos)
                    self.view.showRobotPos(robot_pos)

                    robot_normal = np.matmul(self.R, cam_head_normal)
                    # print(robot_normal)
                    if robot_normal[2] > 0:
                        # 朝下
                        robot_normal = -robot_normal

                    theta = math.pi * 45 / 180  # 若 normal 倾斜超过 theta, 矫正到 theta 内
                    robot_z_vector = np.array([0, 0, -1])
                    if robot_z_vector.dot(robot_normal) < math.cos(theta):
                        nx, ny = robot_normal[0], robot_normal[1]
                        al = math.sin(theta)
                        l = math.sqrt(nx * nx + ny * ny)
                        x, y = al / l * nx, al / l * ny
                        a = np.array([x, y, -math.cos(theta)])
                        robot_normal = a / np.linalg.norm(a)

                    # 末端旋转矩阵
                    TZ = robot_normal
                    if TZ[0] < 1e-5 and TZ[1] < 1e-5:
                        TZ = np.array([0, 0, -1])
                        TX = np.array([-1, 0, 0])
                    else:
                        t = math.sqrt(TZ[0] ** 2 + TZ[2] ** 2)
                        TX = np.array([TZ[2] / t, 0, -TZ[0] / t])
                    TY = np.cross(TZ, TX)

                    M = np.array([TX, TY, TZ]).T

                    R = SSTR.from_matrix(M)
                    rotvet = R.as_rotvec()

                    Rx, Ry, Rz = rotvet[0], rotvet[1], rotvet[2]
                    robot_pos -= 0.1 * robot_normal

                    # print("robot_normal", Rx, Ry, Rz)
                    if USEROBOT and rxmin <= robot_pos[0] <= rxmax and rymin <= robot_pos[1] <= rymax and rzmin <= robot_pos[
                        2] <= rzmax:
                        pos = np.array([robot_pos[0], robot_pos[1], robot_pos[2], Rx, Ry, Rz])

                        self.model.robot.filter.add(pos)

            height, width, channel = color_img.shape
            color_img = cv2.resize(color_img, (width//2, height//2))
            img = QImage(color_img.data, width//2, height//2, width//2*channel, QImage.Format_RGB888)
            self.view.showrgbView(img)

            if self.model.robot_can_move:
                self.view.showDebugInfo("robot运行")
            else:
                self.view.showDebugInfo("robot停止")


class MainWindow(QMainWindow):
    def __init__(self):

        super(MainWindow, self).__init__()
        self.ui = Ui_MainWindow()
        self.ui.setupUi(self)
        self.isclosed = False

        # ----> SET WINDOW TITLE AND ICON
        applicationName = "hello world"
        self.setWindowTitle(applicationName)

        self.vtkWidget = QVTKRenderWindowInteractor(self.ui.lbmodel)  # 创建3D模型加载窗口
        self.vtkWidget.setWindowOpacity(0)  # 设置透明度
        self.vtkWidget.resize(550, 540)  # 设置3D模型大小

        self.ui.lbvideo.setMaximumSize(550, 540)
        self.ui.lbvideo.setPixmap(QPixmap(550, 540))
        self.ui.lbvideo.setScaledContents(True)

        # filename = 'C:/Users/10401/Desktop/00.3DS'
        filename = "humanbrain.3DS"
        importer = vtk.vtk3DSImporter()
        # importer.ComputeNormalsOn()
        importer.SetFileName(filename)
        importer.Read()

        self.ren = importer.GetRenderer()
        self.ren.SetBackground(0.1, 0.2, 0.4)
        self.renwin = self.vtkWidget.GetRenderWindow()
        importer.SetRenderWindow(self.renwin)
        # self.renwin.Render()
        self.iren = self.renwin.GetInteractor()

        self.iren.Initialize()
        self.iren.Start()

        #############################################################

        self.ms = MySignal()
        self.ms.str_msg.connect(self._updateLabel)
        # self.ms.ndarray_msg.connect(self.updateCoordinate)
        # self.ms.int_msg.connect(self.updateBoxValue)
        self.ms.progressbar_int_msg.connect(self._updateProgressBar)
        self.ms.rgbviewer_msg.connect(self._updateView)

    def closeEvent(self, a0: QtGui.QCloseEvent) -> None:
        self.isclosed = True

    def _updateLabel(self, lb, info):
        lb.setText(info)

    def _updateProgressBar(self, pb, val):
        pb.setValue(val)

    def _updateView(self, lb, data):
        pix_img = QPixmap.fromImage(data)
        # pix_img = pix_img.scaled(550, 540, QtCore.Qt.KeepAspectRatio)
        lb.setPixmap(pix_img)
        lb.setScaledContents(True)

    def showrgbView(self, data):
        self.ms.rgbviewer_msg.emit(self.ui.lbvideo, data)

    def showDebugInfo(self, info):
        self.ms.str_msg.emit(self.ui.lb12, str(info))

    def showCameraPos(self, camera_pos):
        if camera_pos is not None:
            msg = str(round(camera_pos[0], 0)) + ", " \
                  + str(round(camera_pos[1], 0)) + ", "\
                  + str(round(camera_pos[2], 0))
        else:
            msg = " - , - , - "
        self.ms.str_msg.emit(self.ui.lb22, msg)

    def showRobotPos(self, robot_pos):
        if robot_pos is not None:
            msg = str(round(1000*robot_pos[0], 0)) + ", "\
                  + str(round(1000*robot_pos[1], 0)) + ", "\
                  + str(round(1000*robot_pos[2], 0))
        else:
            msg = " - , - , - "
        self.ms.str_msg.emit(self.ui.lb32, msg)


if __name__ == "__main__":
    app = QApplication(sys.argv)
    logi = Controller()
    logi.start()
    sys.exit(app.exec_())
